[
    {
        "full_name": "gemma3:1b",
        "name": "gemma3",
        "msg": "The current, most capable model that runs on a single GPU.",
        "size": "815MB",
        "zh_cn_msg": "当前在单个GPU上运行的最强大的模型。",
        "link": "https://ollama.com/library/gemma3:1b",
        "parameters": "1b",
        "pull_count": "3M",
        "tag_count": "17",
        "updated": "8 days ago",
        "updated_time": 1742883756,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "gemma3:4b",
        "name": "gemma3",
        "msg": "The current, most capable model that runs on a single GPU.",
        "size": "3.3GB",
        "zh_cn_msg": "当前在单个GPU上运行的最强大的模型。",
        "link": "https://ollama.com/library/gemma3:4b",
        "parameters": "4b",
        "pull_count": "3M",
        "tag_count": "17",
        "updated": "8 days ago",
        "updated_time": 1742883756,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "gemma3:12b",
        "name": "gemma3",
        "msg": "The current, most capable model that runs on a single GPU.",
        "size": "8.1GB",
        "zh_cn_msg": "当前在单个GPU上运行的最强大的模型。",
        "link": "https://ollama.com/library/gemma3:12b",
        "parameters": "12b",
        "pull_count": "3M",
        "tag_count": "17",
        "updated": "8 days ago",
        "updated_time": 1742883756,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "gemma3:27b",
        "name": "gemma3",
        "msg": "The current, most capable model that runs on a single GPU.",
        "size": "17GB",
        "zh_cn_msg": "当前在单个GPU上运行的最强大的模型。",
        "link": "https://ollama.com/library/gemma3:27b",
        "parameters": "27b",
        "pull_count": "3M",
        "tag_count": "17",
        "updated": "8 days ago",
        "updated_time": 1742883756,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "qwq:32b",
        "name": "qwq",
        "msg": "QwQ is the reasoning model of the Qwen series.",
        "size": "20GB",
        "zh_cn_msg": "QwQ是Qwen系列的推理模型。",
        "link": "https://ollama.com/library/qwq:32b",
        "parameters": "32b",
        "pull_count": "1.2M",
        "tag_count": "8",
        "updated": "2 weeks ago",
        "updated_time": 1742365356,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "deepseek-r1:1.5b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "1.1GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:1.5b",
        "parameters": "1.5b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:7b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "4.7GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:7b",
        "parameters": "7b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:8b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "4.9GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:8b",
        "parameters": "8b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:14b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "9.0GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:14b",
        "parameters": "14b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:32b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "20GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:32b",
        "parameters": "32b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:70b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "43GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:70b",
        "parameters": "70b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-r1:671b",
        "name": "deepseek-r1",
        "msg": "DeepSeek's first-generation of reasoning models with comparable performance to OpenAI-o1, including six dense models distilled from DeepSeek-R1 based on Llama and Qwen.",
        "size": "404GB",
        "zh_cn_msg": "DeepSeek第一代与OpenAI-o1性能相当的推理模型，包括六个基于Llama和Qwen从DeepSeek-R1中蒸馏出的密集模型。",
        "link": "https://ollama.com/library/deepseek-r1:671b",
        "parameters": "671b",
        "pull_count": "32.3M",
        "tag_count": "29",
        "updated": "7 weeks ago",
        "updated_time": 1739341356,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3.3:70b",
        "name": "llama3.3",
        "msg": "New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.",
        "size": "43GB",
        "zh_cn_msg": "最新一代的70B模型。Llama 3.3 70B在性能上与Llama 3.1 405B模型类似。",
        "link": "https://ollama.com/library/llama3.3:70b",
        "parameters": "70b",
        "pull_count": "1.6M",
        "tag_count": "14",
        "updated": "3 months ago",
        "updated_time": 1735798956,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "phi4:14b",
        "name": "phi4",
        "msg": "Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.",
        "size": "9.1GB",
        "zh_cn_msg": "Phi-4 是一个由微软开发的140亿参数的最先进的开源模型。",
        "link": "https://ollama.com/library/phi4:14b",
        "parameters": "14b",
        "pull_count": "1.3M",
        "tag_count": "5",
        "updated": "2 months ago",
        "updated_time": 1738390957,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3.2:1b",
        "name": "llama3.2",
        "msg": "Meta's Llama 3.2 goes small with 1B and 3B models. ",
        "size": "1.3GB",
        "zh_cn_msg": "Meta的Llama 3.2推出了小型版本，包括10亿和30亿参数的模型。",
        "link": "https://ollama.com/library/llama3.2:1b",
        "parameters": "1b",
        "pull_count": "12M",
        "tag_count": "63",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3.2:3b",
        "name": "llama3.2",
        "msg": "Meta's Llama 3.2 goes small with 1B and 3B models. ",
        "size": "2.0GB",
        "zh_cn_msg": "Meta的Llama 3.2推出了小型版本，包括10亿和30亿参数的模型。",
        "link": "https://ollama.com/library/llama3.2:3b",
        "parameters": "3b",
        "pull_count": "12M",
        "tag_count": "63",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3.1:8b",
        "name": "llama3.1",
        "msg": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
        "size": "4.9GB",
        "zh_cn_msg": "Llama 3.1 是Meta公司推出的一种新的最先进的模型，提供80亿、700亿和4050亿参数规模的版本。",
        "link": "https://ollama.com/library/llama3.1:8b",
        "parameters": "8b",
        "pull_count": "76M",
        "tag_count": "93",
        "updated": "4 months ago",
        "updated_time": 1733206957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3.1:70b",
        "name": "llama3.1",
        "msg": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
        "size": "43GB",
        "zh_cn_msg": "Llama 3.1 是Meta公司推出的一种新的最先进的模型，提供80亿、700亿和4050亿参数规模的版本。",
        "link": "https://ollama.com/library/llama3.1:70b",
        "parameters": "70b",
        "pull_count": "76M",
        "tag_count": "93",
        "updated": "4 months ago",
        "updated_time": 1733206957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3.1:405b",
        "name": "llama3.1",
        "msg": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
        "size": "243GB",
        "zh_cn_msg": "Llama 3.1 是Meta公司推出的一种新的最先进的模型，提供80亿、700亿和4050亿参数规模的版本。",
        "link": "https://ollama.com/library/llama3.1:405b",
        "parameters": "405b",
        "pull_count": "76M",
        "tag_count": "93",
        "updated": "4 months ago",
        "updated_time": 1733206957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "nomic-embed-text:latest",
        "name": "nomic-embed-text",
        "msg": "A high-performing open embedding model with a large token context window.",
        "size": "274MB",
        "zh_cn_msg": "高性能的开源嵌入式模型，具有大的令牌上下文窗口。",
        "link": "https://ollama.com/library/nomic-embed-text:latest",
        "parameters": "latest",
        "pull_count": "20.9M",
        "tag_count": "3",
        "updated": "13 months ago",
        "updated_time": 1709878957,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "mistral:7b",
        "name": "mistral",
        "msg": "The 7B model released by Mistral AI, updated to version 0.3.",
        "size": "4.1GB",
        "zh_cn_msg": "由Mistral AI发布的7B模型，更新至版本0.3。",
        "link": "https://ollama.com/library/mistral:7b",
        "parameters": "7b",
        "pull_count": "11.1M",
        "tag_count": "84",
        "updated": "8 months ago",
        "updated_time": 1722838957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3:8b",
        "name": "llama3",
        "msg": "Meta Llama 3: The most capable openly available LLM to date",
        "size": "4.7GB",
        "zh_cn_msg": "Meta Llama 3：迄今为止最强大的公开可用的大语言模型（LLM）",
        "link": "https://ollama.com/library/llama3:8b",
        "parameters": "8b",
        "pull_count": "7.7M",
        "tag_count": "68",
        "updated": "10 months ago",
        "updated_time": 1717654957,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3:70b",
        "name": "llama3",
        "msg": "Meta Llama 3: The most capable openly available LLM to date",
        "size": "40GB",
        "zh_cn_msg": "Meta Llama 3：迄今为止最强大的公开可用的大语言模型（LLM）",
        "link": "https://ollama.com/library/llama3:70b",
        "parameters": "70b",
        "pull_count": "7.7M",
        "tag_count": "68",
        "updated": "10 months ago",
        "updated_time": 1717654957,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen2.5:0.5b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "398MB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:0.5b",
        "parameters": "0.5b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:1.5b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "986MB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:1.5b",
        "parameters": "1.5b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:3b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "1.9GB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:3b",
        "parameters": "3b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:7b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "4.7GB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:7b",
        "parameters": "7b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:14b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "9.0GB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:14b",
        "parameters": "14b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:32b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "20GB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:32b",
        "parameters": "32b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5:72b",
        "name": "qwen2.5",
        "msg": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. ",
        "size": "47GB",
        "zh_cn_msg": "Qwen2.5 模型在阿里最新的大规模数据集上进行了预训练，该数据集包含多达18万亿个token。模型支持最多128K token，并具备多语言支持。",
        "link": "https://ollama.com/library/qwen2.5:72b",
        "parameters": "72b",
        "pull_count": "6M",
        "tag_count": "133",
        "updated": "6 months ago",
        "updated_time": 1728022957,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:0.5b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "531MB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:0.5b",
        "parameters": "0.5b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:1.5b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "986MB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:1.5b",
        "parameters": "1.5b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:3b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "1.9GB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:3b",
        "parameters": "3b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:7b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "4.7GB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:7b",
        "parameters": "7b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:14b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "9.0GB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:14b",
        "parameters": "14b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2.5-coder:32b",
        "name": "qwen2.5-coder",
        "msg": "The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",
        "size": "20GB",
        "zh_cn_msg": "最新的代码特定版本Qwen模型，代码生成、代码推理和代码修复能力有了显著提升。",
        "link": "https://ollama.com/library/qwen2.5-coder:32b",
        "parameters": "32b",
        "pull_count": "4.8M",
        "tag_count": "196",
        "updated": "4 months ago",
        "updated_time": 1733206958,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen:0.5b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "395MB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:0.5b",
        "parameters": "0.5b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:1.8b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "1.1GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:1.8b",
        "parameters": "1.8b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:4b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "2.3GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:4b",
        "parameters": "4b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:7b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "4.5GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:7b",
        "parameters": "7b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:14b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "8.2GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:14b",
        "parameters": "14b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:32b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "18GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:32b",
        "parameters": "32b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:72b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "41GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:72b",
        "parameters": "72b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen:110b",
        "name": "qwen",
        "msg": "Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters",
        "size": "63GB",
        "zh_cn_msg": "Qwen 1.5 是阿里云推出的一系列大型语言模型，参数规模从5亿到1100亿不等。",
        "link": "https://ollama.com/library/qwen:110b",
        "parameters": "110b",
        "pull_count": "4.5M",
        "tag_count": "379",
        "updated": "11 months ago",
        "updated_time": 1715062958,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llava:7b",
        "name": "llava",
        "msg": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
        "size": "4.7GB",
        "zh_cn_msg": "🌋 LLaVA是一个新颖的端到端训练的大规模多模态模型，结合了视觉编码器和Vicuna以实现通用的视觉和语言理解。更新至版本1.6。",
        "link": "https://ollama.com/library/llava:7b",
        "parameters": "7b",
        "pull_count": "4.5M",
        "tag_count": "98",
        "updated": "14 months ago",
        "updated_time": 1707286958,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "llava:13b",
        "name": "llava",
        "msg": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
        "size": "8.0GB",
        "zh_cn_msg": "🌋 LLaVA是一个新颖的端到端训练的大规模多模态模型，结合了视觉编码器和Vicuna以实现通用的视觉和语言理解。更新至版本1.6。",
        "link": "https://ollama.com/library/llava:13b",
        "parameters": "13b",
        "pull_count": "4.5M",
        "tag_count": "98",
        "updated": "14 months ago",
        "updated_time": 1707286958,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "llava:34b",
        "name": "llava",
        "msg": "🌋 LLaVA is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Updated to version 1.6.",
        "size": "20GB",
        "zh_cn_msg": "🌋 LLaVA是一个新颖的端到端训练的大规模多模态模型，结合了视觉编码器和Vicuna以实现通用的视觉和语言理解。更新至版本1.6。",
        "link": "https://ollama.com/library/llava:34b",
        "parameters": "34b",
        "pull_count": "4.5M",
        "tag_count": "98",
        "updated": "14 months ago",
        "updated_time": 1707286958,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "gemma:2b",
        "name": "gemma",
        "msg": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
        "size": "1.7GB",
        "zh_cn_msg": "Gemma是Google DeepMind开发的一系列轻量级、最先进的开源模型。已更新至版本1.1",
        "link": "https://ollama.com/library/gemma:2b",
        "parameters": "2b",
        "pull_count": "4.4M",
        "tag_count": "102",
        "updated": "11 months ago",
        "updated_time": 1715062959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "gemma:7b",
        "name": "gemma",
        "msg": "Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1",
        "size": "5.0GB",
        "zh_cn_msg": "Gemma是Google DeepMind开发的一系列轻量级、最先进的开源模型。已更新至版本1.1",
        "link": "https://ollama.com/library/gemma:7b",
        "parameters": "7b",
        "pull_count": "4.4M",
        "tag_count": "102",
        "updated": "11 months ago",
        "updated_time": 1715062959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen2:0.5b",
        "name": "qwen2",
        "msg": "Qwen2 is a new series of large language models from Alibaba group",
        "size": "352MB",
        "zh_cn_msg": "Qwen2 是阿里集团推出的新一系列大型语言模型。",
        "link": "https://ollama.com/library/qwen2:0.5b",
        "parameters": "0.5b",
        "pull_count": "4.2M",
        "tag_count": "97",
        "updated": "6 months ago",
        "updated_time": 1728022959,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2:1.5b",
        "name": "qwen2",
        "msg": "Qwen2 is a new series of large language models from Alibaba group",
        "size": "935MB",
        "zh_cn_msg": "Qwen2 是阿里集团推出的新一系列大型语言模型。",
        "link": "https://ollama.com/library/qwen2:1.5b",
        "parameters": "1.5b",
        "pull_count": "4.2M",
        "tag_count": "97",
        "updated": "6 months ago",
        "updated_time": 1728022959,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2:7b",
        "name": "qwen2",
        "msg": "Qwen2 is a new series of large language models from Alibaba group",
        "size": "4.4GB",
        "zh_cn_msg": "Qwen2 是阿里集团推出的新一系列大型语言模型。",
        "link": "https://ollama.com/library/qwen2:7b",
        "parameters": "7b",
        "pull_count": "4.2M",
        "tag_count": "97",
        "updated": "6 months ago",
        "updated_time": 1728022959,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "qwen2:72b",
        "name": "qwen2",
        "msg": "Qwen2 is a new series of large language models from Alibaba group",
        "size": "41GB",
        "zh_cn_msg": "Qwen2 是阿里集团推出的新一系列大型语言模型。",
        "link": "https://ollama.com/library/qwen2:72b",
        "parameters": "72b",
        "pull_count": "4.2M",
        "tag_count": "97",
        "updated": "6 months ago",
        "updated_time": 1728022959,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "gemma2:2b",
        "name": "gemma2",
        "msg": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
        "size": "1.6GB",
        "zh_cn_msg": "Google Gemma 2 是一个高性能且高效的模型，提供三种规模：2B、9B 和 27B。",
        "link": "https://ollama.com/library/gemma2:2b",
        "parameters": "2b",
        "pull_count": "3.7M",
        "tag_count": "94",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "gemma2:9b",
        "name": "gemma2",
        "msg": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
        "size": "5.4GB",
        "zh_cn_msg": "Google Gemma 2 是一个高性能且高效的模型，提供三种规模：2B、9B 和 27B。",
        "link": "https://ollama.com/library/gemma2:9b",
        "parameters": "9b",
        "pull_count": "3.7M",
        "tag_count": "94",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "gemma2:27b",
        "name": "gemma2",
        "msg": "Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.",
        "size": "16GB",
        "zh_cn_msg": "Google Gemma 2 是一个高性能且高效的模型，提供三种规模：2B、9B 和 27B。",
        "link": "https://ollama.com/library/gemma2:27b",
        "parameters": "27b",
        "pull_count": "3.7M",
        "tag_count": "94",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2:7b",
        "name": "llama2",
        "msg": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
        "size": "3.8GB",
        "zh_cn_msg": "Llama 2是一系列基础语言模型，参数规模从70亿到700亿不等。",
        "link": "https://ollama.com/library/llama2:7b",
        "parameters": "7b",
        "pull_count": "3.1M",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2:13b",
        "name": "llama2",
        "msg": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
        "size": "7.4GB",
        "zh_cn_msg": "Llama 2是一系列基础语言模型，参数规模从70亿到700亿不等。",
        "link": "https://ollama.com/library/llama2:13b",
        "parameters": "13b",
        "pull_count": "3.1M",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2:70b",
        "name": "llama2",
        "msg": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",
        "size": "39GB",
        "zh_cn_msg": "Llama 2是一系列基础语言模型，参数规模从70亿到700亿不等。",
        "link": "https://ollama.com/library/llama2:70b",
        "parameters": "70b",
        "pull_count": "3.1M",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "phi3:3.8b",
        "name": "phi3",
        "msg": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
        "size": "2.2GB",
        "zh_cn_msg": "Phi-3 是由微软开发的一系列轻量级的先进开源模型家族，包括30亿参数（迷你版）和140亿参数（中型版）。",
        "link": "https://ollama.com/library/phi3:3.8b",
        "parameters": "3.8b",
        "pull_count": "3M",
        "tag_count": "72",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "phi3:14b",
        "name": "phi3",
        "msg": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
        "size": "7.9GB",
        "zh_cn_msg": "Phi-3 是由微软开发的一系列轻量级的先进开源模型家族，包括30亿参数（迷你版）和140亿参数（中型版）。",
        "link": "https://ollama.com/library/phi3:14b",
        "parameters": "14b",
        "pull_count": "3M",
        "tag_count": "72",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mxbai-embed-large:335m",
        "name": "mxbai-embed-large",
        "msg": "State-of-the-art large embedding model from mixedbread.ai",
        "size": "670MB",
        "zh_cn_msg": "mixedbread.ai最新的大型嵌入式模型",
        "link": "https://ollama.com/library/mxbai-embed-large:335m",
        "parameters": "335m",
        "pull_count": "2M",
        "tag_count": "4",
        "updated": "11 months ago",
        "updated_time": 1715062959,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "codellama:7b",
        "name": "codellama",
        "msg": "A large language model that can use text prompts to generate and discuss code.",
        "size": "3.8GB",
        "zh_cn_msg": "一个可以使用文本提示生成和讨论代码的大语言模型。",
        "link": "https://ollama.com/library/codellama:7b",
        "parameters": "7b",
        "pull_count": "1.9M",
        "tag_count": "199",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codellama:13b",
        "name": "codellama",
        "msg": "A large language model that can use text prompts to generate and discuss code.",
        "size": "7.4GB",
        "zh_cn_msg": "一个可以使用文本提示生成和讨论代码的大语言模型。",
        "link": "https://ollama.com/library/codellama:13b",
        "parameters": "13b",
        "pull_count": "1.9M",
        "tag_count": "199",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codellama:34b",
        "name": "codellama",
        "msg": "A large language model that can use text prompts to generate and discuss code.",
        "size": "19GB",
        "zh_cn_msg": "一个可以使用文本提示生成和讨论代码的大语言模型。",
        "link": "https://ollama.com/library/codellama:34b",
        "parameters": "34b",
        "pull_count": "1.9M",
        "tag_count": "199",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codellama:70b",
        "name": "codellama",
        "msg": "A large language model that can use text prompts to generate and discuss code.",
        "size": "39GB",
        "zh_cn_msg": "一个可以使用文本提示生成和讨论代码的大语言模型。",
        "link": "https://ollama.com/library/codellama:70b",
        "parameters": "70b",
        "pull_count": "1.9M",
        "tag_count": "199",
        "updated": "8 months ago",
        "updated_time": 1722838959,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3.2-vision:11b",
        "name": "llama3.2-vision",
        "msg": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
        "size": "7.9GB",
        "zh_cn_msg": "Llama 3.2 Vision 是一组经过指令调优的图像推理生成模型，提供11B和90B两种规模。",
        "link": "https://ollama.com/library/llama3.2-vision:11b",
        "parameters": "11b",
        "pull_count": "1.7M",
        "tag_count": "9",
        "updated": "4 months ago",
        "updated_time": 1733206960,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "llama3.2-vision:90b",
        "name": "llama3.2-vision",
        "msg": "Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.",
        "size": "55GB",
        "zh_cn_msg": "Llama 3.2 Vision 是一组经过指令调优的图像推理生成模型，提供11B和90B两种规模。",
        "link": "https://ollama.com/library/llama3.2-vision:90b",
        "parameters": "90b",
        "pull_count": "1.7M",
        "tag_count": "9",
        "updated": "4 months ago",
        "updated_time": 1733206960,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "mistral-nemo:12b",
        "name": "mistral-nemo",
        "msg": "A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.",
        "size": "7.1GB",
        "zh_cn_msg": "由Mistral AI与NVIDIA合作构建的最先进的120亿参数模型，上下文长度为128k。",
        "link": "https://ollama.com/library/mistral-nemo:12b",
        "parameters": "12b",
        "pull_count": "1.4M",
        "tag_count": "17",
        "updated": "7 months ago",
        "updated_time": 1725430960,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "tinyllama:1.1b",
        "name": "tinyllama",
        "msg": "The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.",
        "size": "638MB",
        "zh_cn_msg": "TinyLlama项目是一个开放的努力，旨在用3万亿个标记来训练一个精简的110亿参数的Llama模型。",
        "link": "https://ollama.com/library/tinyllama:1.1b",
        "parameters": "1.1b",
        "pull_count": "1.4M",
        "tag_count": "36",
        "updated": "15 months ago",
        "updated_time": 1704694960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-v3:671b",
        "name": "deepseek-v3",
        "msg": "A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
        "size": "404GB",
        "zh_cn_msg": "一个强大的专家混合（MoE）语言模型，总参数量为6710亿，每个token激活的参数为370亿。",
        "link": "https://ollama.com/library/deepseek-v3:671b",
        "parameters": "671b",
        "pull_count": "924K",
        "tag_count": "5",
        "updated": "2 months ago",
        "updated_time": 1738390960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder2:3b",
        "name": "starcoder2",
        "msg": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. ",
        "size": "1.7GB",
        "zh_cn_msg": "StarCoder2 是下一代透明训练的开源代码大型语言模型，提供三种规模：30亿、70亿和150亿参数。",
        "link": "https://ollama.com/library/starcoder2:3b",
        "parameters": "3b",
        "pull_count": "909.2K",
        "tag_count": "67",
        "updated": "6 months ago",
        "updated_time": 1728022960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder2:7b",
        "name": "starcoder2",
        "msg": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. ",
        "size": "4.0GB",
        "zh_cn_msg": "StarCoder2 是下一代透明训练的开源代码大型语言模型，提供三种规模：30亿、70亿和150亿参数。",
        "link": "https://ollama.com/library/starcoder2:7b",
        "parameters": "7b",
        "pull_count": "909.2K",
        "tag_count": "67",
        "updated": "6 months ago",
        "updated_time": 1728022960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder2:15b",
        "name": "starcoder2",
        "msg": "StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. ",
        "size": "9.1GB",
        "zh_cn_msg": "StarCoder2 是下一代透明训练的开源代码大型语言模型，提供三种规模：30亿、70亿和150亿参数。",
        "link": "https://ollama.com/library/starcoder2:15b",
        "parameters": "15b",
        "pull_count": "909.2K",
        "tag_count": "67",
        "updated": "6 months ago",
        "updated_time": 1728022960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2-uncensored:7b",
        "name": "llama2-uncensored",
        "msg": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
        "size": "3.8GB",
        "zh_cn_msg": "George Sung和Jarrad Hope发布的未删减版Llama 2模型。",
        "link": "https://ollama.com/library/llama2-uncensored:7b",
        "parameters": "7b",
        "pull_count": "837.4K",
        "tag_count": "34",
        "updated": "17 months ago",
        "updated_time": 1699510960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2-uncensored:70b",
        "name": "llama2-uncensored",
        "msg": "Uncensored Llama 2 model by George Sung and Jarrad Hope.",
        "size": "39GB",
        "zh_cn_msg": "George Sung和Jarrad Hope发布的未删减版Llama 2模型。",
        "link": "https://ollama.com/library/llama2-uncensored:70b",
        "parameters": "70b",
        "pull_count": "837.4K",
        "tag_count": "34",
        "updated": "17 months ago",
        "updated_time": 1699510960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "minicpm-v:8b",
        "name": "minicpm-v",
        "msg": "A series of multimodal LLMs (MLLMs) designed for vision-language understanding.",
        "size": "5.5GB",
        "zh_cn_msg": "一系列用于视觉-语言理解的多模态大型语言模型（MLLMs）。",
        "link": "https://ollama.com/library/minicpm-v:8b",
        "parameters": "8b",
        "pull_count": "784.1K",
        "tag_count": "17",
        "updated": "4 months ago",
        "updated_time": 1733206960,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "bge-m3:567m",
        "name": "bge-m3",
        "msg": "BGE-M3 is a new model from BAAI distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity.",
        "size": "1.2GB",
        "zh_cn_msg": "BGE-M3是来自北京智源人工智能研究院的一个新模型，以其在多功能性、多语言性和多粒度方面的灵活性而著称。",
        "link": "https://ollama.com/library/bge-m3:567m",
        "parameters": "567m",
        "pull_count": "767.4K",
        "tag_count": "3",
        "updated": "7 months ago",
        "updated_time": 1725430960,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "deepseek-coder-v2:16b",
        "name": "deepseek-coder-v2",
        "msg": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
        "size": "8.9GB",
        "zh_cn_msg": "一个开源的专家混合代码语言模型，在特定的代码任务中能达到与GPT4-Turbo相当的性能。",
        "link": "https://ollama.com/library/deepseek-coder-v2:16b",
        "parameters": "16b",
        "pull_count": "759.6K",
        "tag_count": "64",
        "updated": "6 months ago",
        "updated_time": 1728022960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-coder-v2:236b",
        "name": "deepseek-coder-v2",
        "msg": "An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.",
        "size": "133GB",
        "zh_cn_msg": "一个开源的专家混合代码语言模型，在特定的代码任务中能达到与GPT4-Turbo相当的性能。",
        "link": "https://ollama.com/library/deepseek-coder-v2:236b",
        "parameters": "236b",
        "pull_count": "759.6K",
        "tag_count": "64",
        "updated": "6 months ago",
        "updated_time": 1728022960,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed:22m",
        "name": "snowflake-arctic-embed",
        "msg": "A suite of text embedding models by Snowflake, optimized for performance.",
        "size": "46MB",
        "zh_cn_msg": "Snowflake的一系列优化了性能的文本嵌入模型。",
        "link": "https://ollama.com/library/snowflake-arctic-embed:22m",
        "parameters": "22m",
        "pull_count": "703.5K",
        "tag_count": "16",
        "updated": "11 months ago",
        "updated_time": 1715062961,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed:33m",
        "name": "snowflake-arctic-embed",
        "msg": "A suite of text embedding models by Snowflake, optimized for performance.",
        "size": "67MB",
        "zh_cn_msg": "Snowflake的一系列优化了性能的文本嵌入模型。",
        "link": "https://ollama.com/library/snowflake-arctic-embed:33m",
        "parameters": "33m",
        "pull_count": "703.5K",
        "tag_count": "16",
        "updated": "11 months ago",
        "updated_time": 1715062961,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed:110m",
        "name": "snowflake-arctic-embed",
        "msg": "A suite of text embedding models by Snowflake, optimized for performance.",
        "size": "219MB",
        "zh_cn_msg": "Snowflake的一系列优化了性能的文本嵌入模型。",
        "link": "https://ollama.com/library/snowflake-arctic-embed:110m",
        "parameters": "110m",
        "pull_count": "703.5K",
        "tag_count": "16",
        "updated": "11 months ago",
        "updated_time": 1715062961,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed:137m",
        "name": "snowflake-arctic-embed",
        "msg": "A suite of text embedding models by Snowflake, optimized for performance.",
        "size": "274MB",
        "zh_cn_msg": "Snowflake的一系列优化了性能的文本嵌入模型。",
        "link": "https://ollama.com/library/snowflake-arctic-embed:137m",
        "parameters": "137m",
        "pull_count": "703.5K",
        "tag_count": "16",
        "updated": "11 months ago",
        "updated_time": 1715062961,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed:335m",
        "name": "snowflake-arctic-embed",
        "msg": "A suite of text embedding models by Snowflake, optimized for performance.",
        "size": "669MB",
        "zh_cn_msg": "Snowflake的一系列优化了性能的文本嵌入模型。",
        "link": "https://ollama.com/library/snowflake-arctic-embed:335m",
        "parameters": "335m",
        "pull_count": "703.5K",
        "tag_count": "16",
        "updated": "11 months ago",
        "updated_time": 1715062961,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "deepseek-coder:1.3b",
        "name": "deepseek-coder",
        "msg": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "size": "776MB",
        "zh_cn_msg": "DeepSeek Coder 是一个经过两万亿代码和自然语言标记训练的强大编码模型。",
        "link": "https://ollama.com/library/deepseek-coder:1.3b",
        "parameters": "1.3b",
        "pull_count": "634.3K",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-coder:6.7b",
        "name": "deepseek-coder",
        "msg": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "size": "3.8GB",
        "zh_cn_msg": "DeepSeek Coder 是一个经过两万亿代码和自然语言标记训练的强大编码模型。",
        "link": "https://ollama.com/library/deepseek-coder:6.7b",
        "parameters": "6.7b",
        "pull_count": "634.3K",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-coder:33b",
        "name": "deepseek-coder",
        "msg": "DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.",
        "size": "19GB",
        "zh_cn_msg": "DeepSeek Coder 是一个经过两万亿代码和自然语言标记训练的强大编码模型。",
        "link": "https://ollama.com/library/deepseek-coder:33b",
        "parameters": "33b",
        "pull_count": "634.3K",
        "tag_count": "102",
        "updated": "15 months ago",
        "updated_time": 1704694961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mixtral:8x7b",
        "name": "mixtral",
        "msg": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
        "size": "26GB",
        "zh_cn_msg": "由Mistral AI开发的一组参数分别为8x70亿和8x220亿的带有开放权重的专家混合（MoE）模型。",
        "link": "https://ollama.com/library/mixtral:8x7b",
        "parameters": "8x7b",
        "pull_count": "615.9K",
        "tag_count": "70",
        "updated": "3 months ago",
        "updated_time": 1735798961,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "mixtral:8x22b",
        "name": "mixtral",
        "msg": "A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.",
        "size": "80GB",
        "zh_cn_msg": "由Mistral AI开发的一组参数分别为8x70亿和8x220亿的带有开放权重的专家混合（MoE）模型。",
        "link": "https://ollama.com/library/mixtral:8x22b",
        "parameters": "8x22b",
        "pull_count": "615.9K",
        "tag_count": "70",
        "updated": "3 months ago",
        "updated_time": 1735798961,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "dolphin3:8b",
        "name": "dolphin3",
        "msg": "Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of instruct-tuned models designed to be the ultimate general purpose local model, enabling coding, math, agentic, function calling, and general use cases.",
        "size": "4.9GB",
        "zh_cn_msg": "Dolphin 3.0 Llama 3.1 8B 🐬 是 Dolphin 系列指令微调模型的下一代产品，旨在成为终极通用本地模型，支持编码、数学、代理、函数调用和一般应用场景。",
        "link": "https://ollama.com/library/dolphin3:8b",
        "parameters": "8b",
        "pull_count": "592.1K",
        "tag_count": "5",
        "updated": "2 months ago",
        "updated_time": 1738390961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llava-llama3:8b",
        "name": "llava-llama3",
        "msg": "A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.",
        "size": "5.5GB",
        "zh_cn_msg": "一个由Llama 3 Instruct微调而来的LLaVA模型，在多个基准测试中得分更高。",
        "link": "https://ollama.com/library/llava-llama3:8b",
        "parameters": "8b",
        "pull_count": "556.5K",
        "tag_count": "4",
        "updated": "10 months ago",
        "updated_time": 1717654961,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "codegemma:2b",
        "name": "codegemma",
        "msg": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
        "size": "1.6GB",
        "zh_cn_msg": "CodeGemma 是一组强大而轻量级的模型，能够执行各种编码任务，如代码补全、代码生成、自然语言理解、数学推理和指令跟随。",
        "link": "https://ollama.com/library/codegemma:2b",
        "parameters": "2b",
        "pull_count": "554.3K",
        "tag_count": "85",
        "updated": "8 months ago",
        "updated_time": 1722838961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codegemma:7b",
        "name": "codegemma",
        "msg": "CodeGemma is a collection of powerful, lightweight models that can perform a variety of coding tasks like fill-in-the-middle code completion, code generation, natural language understanding, mathematical reasoning, and instruction following.",
        "size": "5.0GB",
        "zh_cn_msg": "CodeGemma 是一组强大而轻量级的模型，能够执行各种编码任务，如代码补全、代码生成、自然语言理解、数学推理和指令跟随。",
        "link": "https://ollama.com/library/codegemma:7b",
        "parameters": "7b",
        "pull_count": "554.3K",
        "tag_count": "85",
        "updated": "8 months ago",
        "updated_time": 1722838961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "olmo2:7b",
        "name": "olmo2",
        "msg": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
        "size": "4.5GB",
        "zh_cn_msg": "OLMo 2 是一组新的 70 亿和 130 亿参数模型，它们是在最多 5T 个标记上训练的。这些模型在性能上与同等规模的完全开源模型相当或更优，并且在英语学术基准测试中也与带有开源权重的模型（如 Llama 3.1）具有竞争力。",
        "link": "https://ollama.com/library/olmo2:7b",
        "parameters": "7b",
        "pull_count": "552.2K",
        "tag_count": "9",
        "updated": "2 months ago",
        "updated_time": 1738390961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "olmo2:13b",
        "name": "olmo2",
        "msg": "OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens. These models are on par with or better than equivalently sized fully open models, and competitive with open-weight models such as Llama 3.1 on English academic benchmarks.",
        "size": "8.4GB",
        "zh_cn_msg": "OLMo 2 是一组新的 70 亿和 130 亿参数模型，它们是在最多 5T 个标记上训练的。这些模型在性能上与同等规模的完全开源模型相当或更优，并且在英语学术基准测试中也与带有开源权重的模型（如 Llama 3.1）具有竞争力。",
        "link": "https://ollama.com/library/olmo2:13b",
        "parameters": "13b",
        "pull_count": "552.2K",
        "tag_count": "9",
        "updated": "2 months ago",
        "updated_time": 1738390961,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "dolphin-mixtral:8x7b",
        "name": "dolphin-mixtral",
        "msg": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
        "size": "26GB",
        "zh_cn_msg": "无审查、基于Mixtral专家混合模型的8x7B和8x22B微调模型，擅长编码任务。由Eric Hartford创建。",
        "link": "https://ollama.com/library/dolphin-mixtral:8x7b",
        "parameters": "8x7b",
        "pull_count": "532.1K",
        "tag_count": "70",
        "updated": "3 months ago",
        "updated_time": 1735798962,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "dolphin-mixtral:8x22b",
        "name": "dolphin-mixtral",
        "msg": "Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of experts models that excels at coding tasks. Created by Eric Hartford.",
        "size": "80GB",
        "zh_cn_msg": "无审查、基于Mixtral专家混合模型的8x7B和8x22B微调模型，擅长编码任务。由Eric Hartford创建。",
        "link": "https://ollama.com/library/dolphin-mixtral:8x22b",
        "parameters": "8x22b",
        "pull_count": "532.1K",
        "tag_count": "70",
        "updated": "3 months ago",
        "updated_time": 1735798962,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "openthinker:7b",
        "name": "openthinker",
        "msg": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
        "size": "4.7GB",
        "zh_cn_msg": "一个完全开源的推理模型家族，基于从DeepSeek-R1中提炼出来的数据集构建。",
        "link": "https://ollama.com/library/openthinker:7b",
        "parameters": "7b",
        "pull_count": "515.8K",
        "tag_count": "9",
        "updated": "6 weeks ago",
        "updated_time": 1739946162,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "openthinker:32b",
        "name": "openthinker",
        "msg": "A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.",
        "size": "20GB",
        "zh_cn_msg": "一个完全开源的推理模型家族，基于从DeepSeek-R1中提炼出来的数据集构建。",
        "link": "https://ollama.com/library/openthinker:32b",
        "parameters": "32b",
        "pull_count": "515.8K",
        "tag_count": "9",
        "updated": "6 weeks ago",
        "updated_time": 1739946162,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "phi:2.7b",
        "name": "phi",
        "msg": "Phi-2: a 2.7B language model by Microsoft Research that demonstrates outstanding reasoning and language understanding capabilities.",
        "size": "1.6GB",
        "zh_cn_msg": "Phi-2：由微软研究院开发的一款27亿参数的语言模型，展示了出色的理由推理和语言理解能力。",
        "link": "https://ollama.com/library/phi:2.7b",
        "parameters": "2.7b",
        "pull_count": "505.1K",
        "tag_count": "18",
        "updated": "15 months ago",
        "updated_time": 1704694962,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "smollm2:135m",
        "name": "smollm2",
        "msg": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
        "size": "271MB",
        "zh_cn_msg": "SmolLM2 是一个紧凑型语言模型系列，提供三种尺寸：1.35亿、3.6亿和17亿参数。",
        "link": "https://ollama.com/library/smollm2:135m",
        "parameters": "135m",
        "pull_count": "481.3K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614962,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "smollm2:360m",
        "name": "smollm2",
        "msg": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
        "size": "726MB",
        "zh_cn_msg": "SmolLM2 是一个紧凑型语言模型系列，提供三种尺寸：1.35亿、3.6亿和17亿参数。",
        "link": "https://ollama.com/library/smollm2:360m",
        "parameters": "360m",
        "pull_count": "481.3K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614962,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "smollm2:1.7b",
        "name": "smollm2",
        "msg": "SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.",
        "size": "1.8GB",
        "zh_cn_msg": "SmolLM2 是一个紧凑型语言模型系列，提供三种尺寸：1.35亿、3.6亿和17亿参数。",
        "link": "https://ollama.com/library/smollm2:1.7b",
        "parameters": "1.7b",
        "pull_count": "481.3K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614962,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "mistral-small:22b",
        "name": "mistral-small",
        "msg": "Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.",
        "size": "13GB",
        "zh_cn_msg": "Mistral Small 3 在低于 70B 参数的“小型”大型语言模型类别中设立了新的基准。",
        "link": "https://ollama.com/library/mistral-small:22b",
        "parameters": "22b",
        "pull_count": "420.1K",
        "tag_count": "21",
        "updated": "2 months ago",
        "updated_time": 1738390962,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "mistral-small:24b",
        "name": "mistral-small",
        "msg": "Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.",
        "size": "14GB",
        "zh_cn_msg": "Mistral Small 3 在低于 70B 参数的“小型”大型语言模型类别中设立了新的基准。",
        "link": "https://ollama.com/library/mistral-small:24b",
        "parameters": "24b",
        "pull_count": "420.1K",
        "tag_count": "21",
        "updated": "2 months ago",
        "updated_time": 1738390962,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "wizardlm2:7b",
        "name": "wizardlm2",
        "msg": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
        "size": "4.1GB",
        "zh_cn_msg": "微软人工智能最新最先进的大型语言模型，在复杂聊天、多语言、推理和代理使用场景中性能得到提升。",
        "link": "https://ollama.com/library/wizardlm2:7b",
        "parameters": "7b",
        "pull_count": "359.7K",
        "tag_count": "22",
        "updated": "11 months ago",
        "updated_time": 1715062962,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizardlm2:8x22b",
        "name": "wizardlm2",
        "msg": "State of the art large language model from Microsoft AI with improved performance on complex chat, multilingual, reasoning and agent use cases.",
        "size": "80GB",
        "zh_cn_msg": "微软人工智能最新最先进的大型语言模型，在复杂聊天、多语言、推理和代理使用场景中性能得到提升。",
        "link": "https://ollama.com/library/wizardlm2:8x22b",
        "parameters": "8x22b",
        "pull_count": "359.7K",
        "tag_count": "22",
        "updated": "11 months ago",
        "updated_time": 1715062962,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "all-minilm:22m",
        "name": "all-minilm",
        "msg": "Embedding models on very large sentence level datasets.",
        "size": "46MB",
        "zh_cn_msg": "在非常大的句子级别数据集上嵌入模型。",
        "link": "https://ollama.com/library/all-minilm:22m",
        "parameters": "22m",
        "pull_count": "354.3K",
        "tag_count": "10",
        "updated": "11 months ago",
        "updated_time": 1715062962,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "all-minilm:33m",
        "name": "all-minilm",
        "msg": "Embedding models on very large sentence level datasets.",
        "size": "67MB",
        "zh_cn_msg": "在非常大的句子级别数据集上嵌入模型。",
        "link": "https://ollama.com/library/all-minilm:33m",
        "parameters": "33m",
        "pull_count": "354.3K",
        "tag_count": "10",
        "updated": "11 months ago",
        "updated_time": 1715062962,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "dolphin-mistral:7b",
        "name": "dolphin-mistral",
        "msg": "The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.",
        "size": "4.1GB",
        "zh_cn_msg": "基于Mistral的未经过滤的Dolphin模型，在编码任务中表现出色。更新至版本2.8。",
        "link": "https://ollama.com/library/dolphin-mistral:7b",
        "parameters": "7b",
        "pull_count": "331.8K",
        "tag_count": "120",
        "updated": "12 months ago",
        "updated_time": 1712470963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "dolphin-llama3:8b",
        "name": "dolphin-llama3",
        "msg": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.",
        "size": "4.7GB",
        "zh_cn_msg": "Dolphin 2.9 是由 Eric Hartford 基于 Llama 3 创建的新模型，提供 8B 和 70B 两种规模，并具备多种指令、对话和编码技能。",
        "link": "https://ollama.com/library/dolphin-llama3:8b",
        "parameters": "8b",
        "pull_count": "305K",
        "tag_count": "53",
        "updated": "10 months ago",
        "updated_time": 1717654963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "dolphin-llama3:70b",
        "name": "dolphin-llama3",
        "msg": "Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on Llama 3 that has a variety of instruction, conversational, and coding skills.",
        "size": "40GB",
        "zh_cn_msg": "Dolphin 2.9 是由 Eric Hartford 基于 Llama 3 创建的新模型，提供 8B 和 70B 两种规模，并具备多种指令、对话和编码技能。",
        "link": "https://ollama.com/library/dolphin-llama3:70b",
        "parameters": "70b",
        "pull_count": "305K",
        "tag_count": "53",
        "updated": "10 months ago",
        "updated_time": 1717654963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "orca-mini:3b",
        "name": "orca-mini",
        "msg": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
        "size": "2.0GB",
        "zh_cn_msg": "一个从30亿参数到700亿参数的通用型模型，适合入门级硬件。",
        "link": "https://ollama.com/library/orca-mini:3b",
        "parameters": "3b",
        "pull_count": "301K",
        "tag_count": "119",
        "updated": "17 months ago",
        "updated_time": 1699510963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "orca-mini:7b",
        "name": "orca-mini",
        "msg": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
        "size": "3.8GB",
        "zh_cn_msg": "一个从30亿参数到700亿参数的通用型模型，适合入门级硬件。",
        "link": "https://ollama.com/library/orca-mini:7b",
        "parameters": "7b",
        "pull_count": "301K",
        "tag_count": "119",
        "updated": "17 months ago",
        "updated_time": 1699510963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "orca-mini:13b",
        "name": "orca-mini",
        "msg": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
        "size": "7.4GB",
        "zh_cn_msg": "一个从30亿参数到700亿参数的通用型模型，适合入门级硬件。",
        "link": "https://ollama.com/library/orca-mini:13b",
        "parameters": "13b",
        "pull_count": "301K",
        "tag_count": "119",
        "updated": "17 months ago",
        "updated_time": 1699510963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "orca-mini:70b",
        "name": "orca-mini",
        "msg": "A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.",
        "size": "39GB",
        "zh_cn_msg": "一个从30亿参数到700亿参数的通用型模型，适合入门级硬件。",
        "link": "https://ollama.com/library/orca-mini:70b",
        "parameters": "70b",
        "pull_count": "301K",
        "tag_count": "119",
        "updated": "17 months ago",
        "updated_time": 1699510963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "command-r:35b",
        "name": "command-r",
        "msg": "Command R is a Large Language Model optimized for conversational interaction and long context tasks.",
        "size": "19GB",
        "zh_cn_msg": "命令R是一款优化了对话交互和长上下文任务的大规模语言模型。",
        "link": "https://ollama.com/library/command-r:35b",
        "parameters": "35b",
        "pull_count": "287.6K",
        "tag_count": "32",
        "updated": "7 months ago",
        "updated_time": 1725430963,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "yi:6b",
        "name": "yi",
        "msg": "Yi 1.5 is a high-performing, bilingual language model.",
        "size": "3.5GB",
        "zh_cn_msg": " Yi 1.5 是一个高性能的双语语言模型。",
        "link": "https://ollama.com/library/yi:6b",
        "parameters": "6b",
        "pull_count": "271.3K",
        "tag_count": "174",
        "updated": "10 months ago",
        "updated_time": 1717654963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yi:9b",
        "name": "yi",
        "msg": "Yi 1.5 is a high-performing, bilingual language model.",
        "size": "5.0GB",
        "zh_cn_msg": " Yi 1.5 是一个高性能的双语语言模型。",
        "link": "https://ollama.com/library/yi:9b",
        "parameters": "9b",
        "pull_count": "271.3K",
        "tag_count": "174",
        "updated": "10 months ago",
        "updated_time": 1717654963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yi:34b",
        "name": "yi",
        "msg": "Yi 1.5 is a high-performing, bilingual language model.",
        "size": "19GB",
        "zh_cn_msg": " Yi 1.5 是一个高性能的双语语言模型。",
        "link": "https://ollama.com/library/yi:34b",
        "parameters": "34b",
        "pull_count": "271.3K",
        "tag_count": "174",
        "updated": "10 months ago",
        "updated_time": 1717654963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "hermes3:3b",
        "name": "hermes3",
        "msg": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
        "size": "2.0GB",
        "zh_cn_msg": "Hermes 3 是Nous Research公司旗舰型Hermes系列大型语言模型的最新版本。",
        "link": "https://ollama.com/library/hermes3:3b",
        "parameters": "3b",
        "pull_count": "270.4K",
        "tag_count": "65",
        "updated": "3 months ago",
        "updated_time": 1735798963,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "hermes3:8b",
        "name": "hermes3",
        "msg": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
        "size": "4.7GB",
        "zh_cn_msg": "Hermes 3 是Nous Research公司旗舰型Hermes系列大型语言模型的最新版本。",
        "link": "https://ollama.com/library/hermes3:8b",
        "parameters": "8b",
        "pull_count": "270.4K",
        "tag_count": "65",
        "updated": "3 months ago",
        "updated_time": 1735798963,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "hermes3:70b",
        "name": "hermes3",
        "msg": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
        "size": "40GB",
        "zh_cn_msg": "Hermes 3 是Nous Research公司旗舰型Hermes系列大型语言模型的最新版本。",
        "link": "https://ollama.com/library/hermes3:70b",
        "parameters": "70b",
        "pull_count": "270.4K",
        "tag_count": "65",
        "updated": "3 months ago",
        "updated_time": 1735798963,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "hermes3:405b",
        "name": "hermes3",
        "msg": "Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research",
        "size": "229GB",
        "zh_cn_msg": "Hermes 3 是Nous Research公司旗舰型Hermes系列大型语言模型的最新版本。",
        "link": "https://ollama.com/library/hermes3:405b",
        "parameters": "405b",
        "pull_count": "270.4K",
        "tag_count": "65",
        "updated": "3 months ago",
        "updated_time": 1735798963,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "phi3.5:3.8b",
        "name": "phi3.5",
        "msg": "A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.",
        "size": "2.2GB",
        "zh_cn_msg": "一个拥有38亿参数的轻量级AI模型，其性能超过了类似规模和更大规模的模型。",
        "link": "https://ollama.com/library/phi3.5:3.8b",
        "parameters": "3.8b",
        "pull_count": "258.1K",
        "tag_count": "17",
        "updated": "6 months ago",
        "updated_time": 1728022963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "zephyr:7b",
        "name": "zephyr",
        "msg": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.",
        "size": "4.1GB",
        "zh_cn_msg": "Zephyr 是一系列经过微调的 Mistral 和 Mixtral 模型，训练它们充当乐于助人的助手。",
        "link": "https://ollama.com/library/zephyr:7b",
        "parameters": "7b",
        "pull_count": "241.2K",
        "tag_count": "40",
        "updated": "11 months ago",
        "updated_time": 1715062963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "zephyr:141b",
        "name": "zephyr",
        "msg": "Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.",
        "size": "80GB",
        "zh_cn_msg": "Zephyr 是一系列经过微调的 Mistral 和 Mixtral 模型，训练它们充当乐于助人的助手。",
        "link": "https://ollama.com/library/zephyr:141b",
        "parameters": "141b",
        "pull_count": "241.2K",
        "tag_count": "40",
        "updated": "11 months ago",
        "updated_time": 1715062963,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codestral:22b",
        "name": "codestral",
        "msg": "Codestral is Mistral AI’s first-ever code model designed for code generation tasks.",
        "size": "13GB",
        "zh_cn_msg": "Codestral是Mistral AI首个专为代码生成任务设计的代码模型。",
        "link": "https://ollama.com/library/codestral:22b",
        "parameters": "22b",
        "pull_count": "239K",
        "tag_count": "17",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "smollm:135m",
        "name": "smollm",
        "msg": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
        "size": "92MB",
        "zh_cn_msg": "一个参数量为135M、360M和1.7B的系列小型模型，这些模型是在一个新的高质量数据集上训练的。",
        "link": "https://ollama.com/library/smollm:135m",
        "parameters": "135m",
        "pull_count": "205.9K",
        "tag_count": "94",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "smollm:360m",
        "name": "smollm",
        "msg": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
        "size": "229MB",
        "zh_cn_msg": "一个参数量为135M、360M和1.7B的系列小型模型，这些模型是在一个新的高质量数据集上训练的。",
        "link": "https://ollama.com/library/smollm:360m",
        "parameters": "360m",
        "pull_count": "205.9K",
        "tag_count": "94",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "smollm:1.7b",
        "name": "smollm",
        "msg": "🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.",
        "size": "991MB",
        "zh_cn_msg": "一个参数量为135M、360M和1.7B的系列小型模型，这些模型是在一个新的高质量数据集上训练的。",
        "link": "https://ollama.com/library/smollm:1.7b",
        "parameters": "1.7b",
        "pull_count": "205.9K",
        "tag_count": "94",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite-code:3b",
        "name": "granite-code",
        "msg": "A family of open foundation models by IBM for Code Intelligence",
        "size": "2.0GB",
        "zh_cn_msg": "IBM用于代码智能的开源基础模型系列",
        "link": "https://ollama.com/library/granite-code:3b",
        "parameters": "3b",
        "pull_count": "197K",
        "tag_count": "162",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite-code:8b",
        "name": "granite-code",
        "msg": "A family of open foundation models by IBM for Code Intelligence",
        "size": "4.6GB",
        "zh_cn_msg": "IBM用于代码智能的开源基础模型系列",
        "link": "https://ollama.com/library/granite-code:8b",
        "parameters": "8b",
        "pull_count": "197K",
        "tag_count": "162",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite-code:20b",
        "name": "granite-code",
        "msg": "A family of open foundation models by IBM for Code Intelligence",
        "size": "12GB",
        "zh_cn_msg": "IBM用于代码智能的开源基础模型系列",
        "link": "https://ollama.com/library/granite-code:20b",
        "parameters": "20b",
        "pull_count": "197K",
        "tag_count": "162",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite-code:34b",
        "name": "granite-code",
        "msg": "A family of open foundation models by IBM for Code Intelligence",
        "size": "19GB",
        "zh_cn_msg": "IBM用于代码智能的开源基础模型系列",
        "link": "https://ollama.com/library/granite-code:34b",
        "parameters": "34b",
        "pull_count": "197K",
        "tag_count": "162",
        "updated": "7 months ago",
        "updated_time": 1725430964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-vicuna-uncensored:7b",
        "name": "wizard-vicuna-uncensored",
        "msg": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
        "size": "3.8GB",
        "zh_cn_msg": "Wizard Vicuna 未删减版是一个基于Eric Hartford未删减的Llama 2的70亿、130亿和300亿参数模型。",
        "link": "https://ollama.com/library/wizard-vicuna-uncensored:7b",
        "parameters": "7b",
        "pull_count": "191.1K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-vicuna-uncensored:13b",
        "name": "wizard-vicuna-uncensored",
        "msg": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
        "size": "7.4GB",
        "zh_cn_msg": "Wizard Vicuna 未删减版是一个基于Eric Hartford未删减的Llama 2的70亿、130亿和300亿参数模型。",
        "link": "https://ollama.com/library/wizard-vicuna-uncensored:13b",
        "parameters": "13b",
        "pull_count": "191.1K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-vicuna-uncensored:30b",
        "name": "wizard-vicuna-uncensored",
        "msg": "Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.",
        "size": "18GB",
        "zh_cn_msg": "Wizard Vicuna 未删减版是一个基于Eric Hartford未删减的Llama 2的70亿、130亿和300亿参数模型。",
        "link": "https://ollama.com/library/wizard-vicuna-uncensored:30b",
        "parameters": "30b",
        "pull_count": "191.1K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder:1b",
        "name": "starcoder",
        "msg": "StarCoder is a code generation model trained on 80+ programming languages.",
        "size": "726MB",
        "zh_cn_msg": "StarCoder 是一个在 80 多种编程语言上训练的代码生成模型。",
        "link": "https://ollama.com/library/starcoder:1b",
        "parameters": "1b",
        "pull_count": "190.6K",
        "tag_count": "100",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder:3b",
        "name": "starcoder",
        "msg": "StarCoder is a code generation model trained on 80+ programming languages.",
        "size": "1.8GB",
        "zh_cn_msg": "StarCoder 是一个在 80 多种编程语言上训练的代码生成模型。",
        "link": "https://ollama.com/library/starcoder:3b",
        "parameters": "3b",
        "pull_count": "190.6K",
        "tag_count": "100",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder:7b",
        "name": "starcoder",
        "msg": "StarCoder is a code generation model trained on 80+ programming languages.",
        "size": "4.3GB",
        "zh_cn_msg": "StarCoder 是一个在 80 多种编程语言上训练的代码生成模型。",
        "link": "https://ollama.com/library/starcoder:7b",
        "parameters": "7b",
        "pull_count": "190.6K",
        "tag_count": "100",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starcoder:15b",
        "name": "starcoder",
        "msg": "StarCoder is a code generation model trained on 80+ programming languages.",
        "size": "9.0GB",
        "zh_cn_msg": "StarCoder 是一个在 80 多种编程语言上训练的代码生成模型。",
        "link": "https://ollama.com/library/starcoder:15b",
        "parameters": "15b",
        "pull_count": "190.6K",
        "tag_count": "100",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "vicuna:7b",
        "name": "vicuna",
        "msg": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
        "size": "3.8GB",
        "zh_cn_msg": "基于Llama和Llama 2的通用聊天模型，上下文大小为2K到16K。",
        "link": "https://ollama.com/library/vicuna:7b",
        "parameters": "7b",
        "pull_count": "178.1K",
        "tag_count": "111",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "vicuna:13b",
        "name": "vicuna",
        "msg": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Llama和Llama 2的通用聊天模型，上下文大小为2K到16K。",
        "link": "https://ollama.com/library/vicuna:13b",
        "parameters": "13b",
        "pull_count": "178.1K",
        "tag_count": "111",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "vicuna:33b",
        "name": "vicuna",
        "msg": "General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.",
        "size": "18GB",
        "zh_cn_msg": "基于Llama和Llama 2的通用聊天模型，上下文大小为2K到16K。",
        "link": "https://ollama.com/library/vicuna:33b",
        "parameters": "33b",
        "pull_count": "178.1K",
        "tag_count": "111",
        "updated": "17 months ago",
        "updated_time": 1699510964,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mistral-openorca:7b",
        "name": "mistral-openorca",
        "msg": "Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.",
        "size": "4.1GB",
        "zh_cn_msg": "Mistral OpenOrca 是一个具有70亿参数的模型，在 Mistral 7B 模型的基础上，使用 OpenOrca 数据集进行了微调。",
        "link": "https://ollama.com/library/mistral-openorca:7b",
        "parameters": "7b",
        "pull_count": "168.1K",
        "tag_count": "17",
        "updated": "17 months ago",
        "updated_time": 1699510965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "moondream:1.8b",
        "name": "moondream",
        "msg": "moondream2 is a small vision language model designed to run efficiently on edge devices.",
        "size": "1.7GB",
        "zh_cn_msg": "moondream2 是一个小型视觉语言模型，设计上能够高效地在边缘设备上运行。",
        "link": "https://ollama.com/library/moondream:1.8b",
        "parameters": "1.8b",
        "pull_count": "162.7K",
        "tag_count": "18",
        "updated": "10 months ago",
        "updated_time": 1717654965,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "llama2-chinese:7b",
        "name": "llama2-chinese",
        "msg": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
        "size": "3.8GB",
        "zh_cn_msg": "基于Llama 2的模型，经过微调以提升中文对话能力。",
        "link": "https://ollama.com/library/llama2-chinese:7b",
        "parameters": "7b",
        "pull_count": "151.1K",
        "tag_count": "35",
        "updated": "17 months ago",
        "updated_time": 1699510965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama2-chinese:13b",
        "name": "llama2-chinese",
        "msg": "Llama 2 based model fine tuned to improve Chinese dialogue ability.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Llama 2的模型，经过微调以提升中文对话能力。",
        "link": "https://ollama.com/library/llama2-chinese:13b",
        "parameters": "13b",
        "pull_count": "151.1K",
        "tag_count": "35",
        "updated": "17 months ago",
        "updated_time": 1699510965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "openchat:7b",
        "name": "openchat",
        "msg": "A family of open-source models trained on a wide variety of data, surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.",
        "size": "4.1GB",
        "zh_cn_msg": "一个基于开源的模型系列，该系列在广泛的数据上进行了训练，在各种基准测试中超越了ChatGPT。更新至版本3.5-0106。",
        "link": "https://ollama.com/library/openchat:7b",
        "parameters": "7b",
        "pull_count": "149.2K",
        "tag_count": "50",
        "updated": "14 months ago",
        "updated_time": 1707286965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codegeex4:9b",
        "name": "codegeex4",
        "msg": "A versatile model for AI software development scenarios, including code completion.",
        "size": "5.5GB",
        "zh_cn_msg": "一个适用于AI软件开发场景的多功能模型，包括代码完成。",
        "link": "https://ollama.com/library/codegeex4:9b",
        "parameters": "9b",
        "pull_count": "141.4K",
        "tag_count": "17",
        "updated": "8 months ago",
        "updated_time": 1722838965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "aya:8b",
        "name": "aya",
        "msg": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. ",
        "size": "4.8GB",
        "zh_cn_msg": "Cohere发布的Aya 23是一款新的最先进的多语言模型家族，支持23种语言。",
        "link": "https://ollama.com/library/aya:8b",
        "parameters": "8b",
        "pull_count": "138.9K",
        "tag_count": "33",
        "updated": "10 months ago",
        "updated_time": 1717654965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "aya:35b",
        "name": "aya",
        "msg": "Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. ",
        "size": "20GB",
        "zh_cn_msg": "Cohere发布的Aya 23是一款新的最先进的多语言模型家族，支持23种语言。",
        "link": "https://ollama.com/library/aya:35b",
        "parameters": "35b",
        "pull_count": "138.9K",
        "tag_count": "33",
        "updated": "10 months ago",
        "updated_time": 1717654965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codeqwen:7b",
        "name": "codeqwen",
        "msg": "CodeQwen1.5 is a large language model pretrained on a large amount of code data.",
        "size": "4.2GB",
        "zh_cn_msg": "CodeQwen1.5 是一个在大量代码数据上预训练的大语言模型。",
        "link": "https://ollama.com/library/codeqwen:7b",
        "parameters": "7b",
        "pull_count": "138K",
        "tag_count": "30",
        "updated": "9 months ago",
        "updated_time": 1720246965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "openhermes:latest",
        "name": "openhermes",
        "msg": "OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.",
        "size": "4.1GB",
        "zh_cn_msg": "OpenHermes 2.5 是一个由 Teknium 在 Mistral 上使用完全开源的数据集微调的 70 亿参数模型。",
        "link": "https://ollama.com/library/openhermes:latest",
        "parameters": "latest",
        "pull_count": "137.5K",
        "tag_count": "35",
        "updated": "15 months ago",
        "updated_time": 1704694965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-llm:7b",
        "name": "deepseek-llm",
        "msg": "An advanced language model crafted with 2 trillion bilingual tokens.",
        "size": "4.0GB",
        "zh_cn_msg": "一个使用2万亿双语令牌打造的高级语言模型。",
        "link": "https://ollama.com/library/deepseek-llm:7b",
        "parameters": "7b",
        "pull_count": "137.2K",
        "tag_count": "64",
        "updated": "15 months ago",
        "updated_time": 1704694965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-llm:67b",
        "name": "deepseek-llm",
        "msg": "An advanced language model crafted with 2 trillion bilingual tokens.",
        "size": "38GB",
        "zh_cn_msg": "一个使用2万亿双语令牌打造的高级语言模型。",
        "link": "https://ollama.com/library/deepseek-llm:67b",
        "parameters": "67b",
        "pull_count": "137.2K",
        "tag_count": "64",
        "updated": "15 months ago",
        "updated_time": 1704694965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-v2:16b",
        "name": "deepseek-v2",
        "msg": "A strong, economical, and efficient Mixture-of-Experts language model.",
        "size": "8.9GB",
        "zh_cn_msg": "一个强大、经济且高效的专家混合语言模型。",
        "link": "https://ollama.com/library/deepseek-v2:16b",
        "parameters": "16b",
        "pull_count": "134.8K",
        "tag_count": "34",
        "updated": "9 months ago",
        "updated_time": 1720246965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepseek-v2:236b",
        "name": "deepseek-v2",
        "msg": "A strong, economical, and efficient Mixture-of-Experts language model.",
        "size": "133GB",
        "zh_cn_msg": "一个强大、经济且高效的专家混合语言模型。",
        "link": "https://ollama.com/library/deepseek-v2:236b",
        "parameters": "236b",
        "pull_count": "134.8K",
        "tag_count": "34",
        "updated": "9 months ago",
        "updated_time": 1720246965,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mistral-large:123b",
        "name": "mistral-large",
        "msg": "Mistral Large 2 is Mistral's new flagship model that is significantly more capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.",
        "size": "73GB",
        "zh_cn_msg": "Mistral Large 2 是 Mistral 的新款旗舰模型，其在代码生成、数学和推理方面的能力显著提升，拥有 128k 上下文窗口，并支持数十种语言。",
        "link": "https://ollama.com/library/mistral-large:123b",
        "parameters": "123b",
        "pull_count": "128.8K",
        "tag_count": "32",
        "updated": "4 months ago",
        "updated_time": 1733206966,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "glm4:9b",
        "name": "glm4",
        "msg": "A strong multi-lingual general language model with competitive performance to Llama 3.",
        "size": "5.5GB",
        "zh_cn_msg": "具有竞争力的多语言通用语言模型，性能与Llama 3相当。",
        "link": "https://ollama.com/library/glm4:9b",
        "parameters": "9b",
        "pull_count": "126.6K",
        "tag_count": "32",
        "updated": "8 months ago",
        "updated_time": 1722838966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "stable-code:3b",
        "name": "stable-code",
        "msg": "Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.",
        "size": "1.6GB",
        "zh_cn_msg": "Stable Code 3B 是一个编码模型，具有指令和代码补全变体，与规模大2.5倍的模型（如Code Llama 7B）相当。",
        "link": "https://ollama.com/library/stable-code:3b",
        "parameters": "3b",
        "pull_count": "124.9K",
        "tag_count": "36",
        "updated": "12 months ago",
        "updated_time": 1712470966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nous-hermes2:10.7b",
        "name": "nous-hermes2",
        "msg": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
        "size": "6.1GB",
        "zh_cn_msg": "Nous Research打造的一系列强大的模型，在科学讨论和编码任务方面表现出色。",
        "link": "https://ollama.com/library/nous-hermes2:10.7b",
        "parameters": "10.7b",
        "pull_count": "123.1K",
        "tag_count": "33",
        "updated": "15 months ago",
        "updated_time": 1704694966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nous-hermes2:34b",
        "name": "nous-hermes2",
        "msg": "The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.",
        "size": "19GB",
        "zh_cn_msg": "Nous Research打造的一系列强大的模型，在科学讨论和编码任务方面表现出色。",
        "link": "https://ollama.com/library/nous-hermes2:34b",
        "parameters": "34b",
        "pull_count": "123.1K",
        "tag_count": "33",
        "updated": "15 months ago",
        "updated_time": 1704694966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen2-math:1.5b",
        "name": "qwen2-math",
        "msg": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
        "size": "935MB",
        "zh_cn_msg": "Qwen2 Math 是一系列基于 Qwen2 大型语言模型的专业数学模型，其在数学能力上远远超过了开源模型甚至闭源模型（例如 GPT4o）。",
        "link": "https://ollama.com/library/qwen2-math:1.5b",
        "parameters": "1.5b",
        "pull_count": "123K",
        "tag_count": "52",
        "updated": "7 months ago",
        "updated_time": 1725430966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen2-math:7b",
        "name": "qwen2-math",
        "msg": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
        "size": "4.4GB",
        "zh_cn_msg": "Qwen2 Math 是一系列基于 Qwen2 大型语言模型的专业数学模型，其在数学能力上远远超过了开源模型甚至闭源模型（例如 GPT4o）。",
        "link": "https://ollama.com/library/qwen2-math:7b",
        "parameters": "7b",
        "pull_count": "123K",
        "tag_count": "52",
        "updated": "7 months ago",
        "updated_time": 1725430966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "qwen2-math:72b",
        "name": "qwen2-math",
        "msg": "Qwen2 Math is a series of specialized math language models built upon the Qwen2 LLMs, which significantly outperforms the mathematical capabilities of open-source models and even closed-source models (e.g., GPT4o).",
        "size": "41GB",
        "zh_cn_msg": "Qwen2 Math 是一系列基于 Qwen2 大型语言模型的专业数学模型，其在数学能力上远远超过了开源模型甚至闭源模型（例如 GPT4o）。",
        "link": "https://ollama.com/library/qwen2-math:72b",
        "parameters": "72b",
        "pull_count": "123K",
        "tag_count": "52",
        "updated": "7 months ago",
        "updated_time": 1725430966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "tinydolphin:1.1b",
        "name": "tinydolphin",
        "msg": "An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.",
        "size": "637MB",
        "zh_cn_msg": "一个由Eric Hartford基于TinyLlama并在新的Dolphin 2.8数据集上训练的实验性11亿参数模型。",
        "link": "https://ollama.com/library/tinydolphin:1.1b",
        "parameters": "1.1b",
        "pull_count": "122.9K",
        "tag_count": "18",
        "updated": "14 months ago",
        "updated_time": 1707286966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "command-r-plus:104b",
        "name": "command-r-plus",
        "msg": "Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.",
        "size": "59GB",
        "zh_cn_msg": "命令R+是一款强大且可扩展的大语言模型，专为在现实世界的企业的应用场景中表现出色而设计。",
        "link": "https://ollama.com/library/command-r-plus:104b",
        "parameters": "104b",
        "pull_count": "121.1K",
        "tag_count": "21",
        "updated": "7 months ago",
        "updated_time": 1725430966,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "wizardcoder:33b",
        "name": "wizardcoder",
        "msg": "State-of-the-art code generation model",
        "size": "19GB",
        "zh_cn_msg": "最先进的代码生成模型",
        "link": "https://ollama.com/library/wizardcoder:33b",
        "parameters": "33b",
        "pull_count": "119K",
        "tag_count": "67",
        "updated": "15 months ago",
        "updated_time": 1704694966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "bakllava:7b",
        "name": "bakllava",
        "msg": "BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.",
        "size": "4.7GB",
        "zh_cn_msg": "BakLLaVA 是一个多模态模型，由 Mistral 7B 基础模型与 LLaVA 架构增强而成。",
        "link": "https://ollama.com/library/bakllava:7b",
        "parameters": "7b",
        "pull_count": "112.3K",
        "tag_count": "17",
        "updated": "15 months ago",
        "updated_time": 1704694966,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "stablelm2:1.6b",
        "name": "stablelm2",
        "msg": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
        "size": "983MB",
        "zh_cn_msg": "Stable LM 2 是一个最先进的语言模型，拥有16亿和120亿参数，在英语、西班牙语、德语、意大利语、法语、葡萄牙语和荷兰语的多语言数据上进行了训练。",
        "link": "https://ollama.com/library/stablelm2:1.6b",
        "parameters": "1.6b",
        "pull_count": "109.8K",
        "tag_count": "84",
        "updated": "11 months ago",
        "updated_time": 1715062966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "stablelm2:12b",
        "name": "stablelm2",
        "msg": "Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.",
        "size": "7.0GB",
        "zh_cn_msg": "Stable LM 2 是一个最先进的语言模型，拥有16亿和120亿参数，在英语、西班牙语、德语、意大利语、法语、葡萄牙语和荷兰语的多语言数据上进行了训练。",
        "link": "https://ollama.com/library/stablelm2:12b",
        "parameters": "12b",
        "pull_count": "109.8K",
        "tag_count": "84",
        "updated": "11 months ago",
        "updated_time": 1715062966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "neural-chat:7b",
        "name": "neural-chat",
        "msg": "A fine-tuned model based on Mistral with good coverage of domain and language.",
        "size": "4.1GB",
        "zh_cn_msg": "基于Mistral的微调模型，涵盖了良好的领域和语言覆盖率。",
        "link": "https://ollama.com/library/neural-chat:7b",
        "parameters": "7b",
        "pull_count": "106.6K",
        "tag_count": "50",
        "updated": "15 months ago",
        "updated_time": 1704694966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "reflection:70b",
        "name": "reflection",
        "msg": "A high-performing model trained with a new technique called Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.",
        "size": "40GB",
        "zh_cn_msg": "一个采用称为Reflection-tuning的新技术训练的高性能模型，该技术教导大型语言模型检测其推理中的错误并进行纠正。",
        "link": "https://ollama.com/library/reflection:70b",
        "parameters": "70b",
        "pull_count": "104.2K",
        "tag_count": "17",
        "updated": "6 months ago",
        "updated_time": 1728022966,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-math:7b",
        "name": "wizard-math",
        "msg": "Model focused on math and logic problems",
        "size": "4.1GB",
        "zh_cn_msg": "专注于数学和逻辑问题的模型",
        "link": "https://ollama.com/library/wizard-math:7b",
        "parameters": "7b",
        "pull_count": "102.2K",
        "tag_count": "64",
        "updated": "15 months ago",
        "updated_time": 1704694967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-math:13b",
        "name": "wizard-math",
        "msg": "Model focused on math and logic problems",
        "size": "7.4GB",
        "zh_cn_msg": "专注于数学和逻辑问题的模型",
        "link": "https://ollama.com/library/wizard-math:13b",
        "parameters": "13b",
        "pull_count": "102.2K",
        "tag_count": "64",
        "updated": "15 months ago",
        "updated_time": 1704694967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-math:70b",
        "name": "wizard-math",
        "msg": "Model focused on math and logic problems",
        "size": "39GB",
        "zh_cn_msg": "专注于数学和逻辑问题的模型",
        "link": "https://ollama.com/library/wizard-math:70b",
        "parameters": "70b",
        "pull_count": "102.2K",
        "tag_count": "64",
        "updated": "15 months ago",
        "updated_time": 1704694967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3-chatqa:8b",
        "name": "llama3-chatqa",
        "msg": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
        "size": "4.7GB",
        "zh_cn_msg": "NVIDIA 基于 Llama 3 的模型，在对话式问题回答（QA）和增强检索生成（RAG）方面表现出色。",
        "link": "https://ollama.com/library/llama3-chatqa:8b",
        "parameters": "8b",
        "pull_count": "99.7K",
        "tag_count": "35",
        "updated": "10 months ago",
        "updated_time": 1717654967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3-chatqa:70b",
        "name": "llama3-chatqa",
        "msg": "A model from NVIDIA based on Llama 3 that excels at conversational question answering (QA) and retrieval-augmented generation (RAG).",
        "size": "40GB",
        "zh_cn_msg": "NVIDIA 基于 Llama 3 的模型，在对话式问题回答（QA）和增强检索生成（RAG）方面表现出色。",
        "link": "https://ollama.com/library/llama3-chatqa:70b",
        "parameters": "70b",
        "pull_count": "99.7K",
        "tag_count": "35",
        "updated": "10 months ago",
        "updated_time": 1717654967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3-gradient:8b",
        "name": "llama3-gradient",
        "msg": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
        "size": "4.7GB",
        "zh_cn_msg": "该模型将LLama-3 8B的上下文长度从8k扩展到超过100万令牌。",
        "link": "https://ollama.com/library/llama3-gradient:8b",
        "parameters": "8b",
        "pull_count": "99.3K",
        "tag_count": "35",
        "updated": "11 months ago",
        "updated_time": 1715062967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3-gradient:70b",
        "name": "llama3-gradient",
        "msg": "This model extends LLama-3 8B's context length from 8k to over 1m tokens.",
        "size": "40GB",
        "zh_cn_msg": "该模型将LLama-3 8B的上下文长度从8k扩展到超过100万令牌。",
        "link": "https://ollama.com/library/llama3-gradient:70b",
        "parameters": "70b",
        "pull_count": "99.3K",
        "tag_count": "35",
        "updated": "11 months ago",
        "updated_time": 1715062967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "sqlcoder:7b",
        "name": "sqlcoder",
        "msg": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
        "size": "4.1GB",
        "zh_cn_msg": "SQLCoder 是一个在 StarCoder 上微调的代码完成模型，专门用于生成 SQL 语句任务。",
        "link": "https://ollama.com/library/sqlcoder:7b",
        "parameters": "7b",
        "pull_count": "98.4K",
        "tag_count": "48",
        "updated": "14 months ago",
        "updated_time": 1707286967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "sqlcoder:15b",
        "name": "sqlcoder",
        "msg": "SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks",
        "size": "9.0GB",
        "zh_cn_msg": "SQLCoder 是一个在 StarCoder 上微调的代码完成模型，专门用于生成 SQL 语句任务。",
        "link": "https://ollama.com/library/sqlcoder:15b",
        "parameters": "15b",
        "pull_count": "98.4K",
        "tag_count": "48",
        "updated": "14 months ago",
        "updated_time": 1707286967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "bge-large:335m",
        "name": "bge-large",
        "msg": "Embedding model from BAAI mapping texts to vectors.",
        "size": "671MB",
        "zh_cn_msg": "来自BAAI的嵌入模型将文本映射到向量。",
        "link": "https://ollama.com/library/bge-large:335m",
        "parameters": "335m",
        "pull_count": "94.7K",
        "tag_count": "3",
        "updated": "7 months ago",
        "updated_time": 1725430967,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "samantha-mistral:7b",
        "name": "samantha-mistral",
        "msg": "A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.",
        "size": "4.1GB",
        "zh_cn_msg": "一个接受过哲学、心理学和个人关系训练的伴侣助手。基于Mistral。",
        "link": "https://ollama.com/library/samantha-mistral:7b",
        "parameters": "7b",
        "pull_count": "91.2K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "phi4-mini:3.8b",
        "name": "phi4-mini",
        "msg": "Phi-4-mini brings significant enhancements in multilingual support, reasoning, and mathematics, and now, the long-awaited function calling feature is finally supported.",
        "size": "2.5GB",
        "zh_cn_msg": "Phi-4-mini 在多语言支持、推理和数学方面带来了显著增强，现在备受期待的功能调用特性终于得到了支持。",
        "link": "https://ollama.com/library/phi4-mini:3.8b",
        "parameters": "3.8b",
        "pull_count": "90.8K",
        "tag_count": "5",
        "updated": "4 weeks ago",
        "updated_time": 1741155767,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3.1-dense:2b",
        "name": "granite3.1-dense",
        "msg": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.",
        "size": "1.6GB",
        "zh_cn_msg": "IBM的Granite 2B和8B模型是仅包含文本的密集型大语言模型，它们在超过12万亿个标记的数据上进行了训练，在IBM最初的测试中，这些模型在性能和速度方面均优于其前代产品。",
        "link": "https://ollama.com/library/granite3.1-dense:2b",
        "parameters": "2b",
        "pull_count": "86.8K",
        "tag_count": "33",
        "updated": "2 months ago",
        "updated_time": 1738390967,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3.1-dense:8b",
        "name": "granite3.1-dense",
        "msg": "The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.",
        "size": "5.0GB",
        "zh_cn_msg": "IBM的Granite 2B和8B模型是仅包含文本的密集型大语言模型，它们在超过12万亿个标记的数据上进行了训练，在IBM最初的测试中，这些模型在性能和速度方面均优于其前代产品。",
        "link": "https://ollama.com/library/granite3.1-dense:8b",
        "parameters": "8b",
        "pull_count": "86.8K",
        "tag_count": "33",
        "updated": "2 months ago",
        "updated_time": 1738390967,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "dolphincoder:7b",
        "name": "dolphincoder",
        "msg": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
        "size": "4.2GB",
        "zh_cn_msg": "Dolphin模型家族的7B和15B未经过滤版本，擅长编码，基于StarCoder2。",
        "link": "https://ollama.com/library/dolphincoder:7b",
        "parameters": "7b",
        "pull_count": "85.9K",
        "tag_count": "35",
        "updated": "11 months ago",
        "updated_time": 1715062967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "dolphincoder:15b",
        "name": "dolphincoder",
        "msg": "A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.",
        "size": "9.1GB",
        "zh_cn_msg": "Dolphin模型家族的7B和15B未经过滤版本，擅长编码，基于StarCoder2。",
        "link": "https://ollama.com/library/dolphincoder:15b",
        "parameters": "15b",
        "pull_count": "85.9K",
        "tag_count": "35",
        "updated": "11 months ago",
        "updated_time": 1715062967,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "xwinlm:7b",
        "name": "xwinlm",
        "msg": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
        "size": "3.8GB",
        "zh_cn_msg": "基于Llama 2的对话模型，在各种基准测试中表现出色。",
        "link": "https://ollama.com/library/xwinlm:7b",
        "parameters": "7b",
        "pull_count": "85.1K",
        "tag_count": "80",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "xwinlm:13b",
        "name": "xwinlm",
        "msg": "Conversational model based on Llama 2 that performs competitively on various benchmarks.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Llama 2的对话模型，在各种基准测试中表现出色。",
        "link": "https://ollama.com/library/xwinlm:13b",
        "parameters": "13b",
        "pull_count": "85.1K",
        "tag_count": "80",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llava-phi3:3.8b",
        "name": "llava-phi3",
        "msg": "A new small LLaVA model fine-tuned from Phi 3 Mini.",
        "size": "2.9GB",
        "zh_cn_msg": "一个新的从Phi 3 Mini微调而来的小型LLaVA模型。",
        "link": "https://ollama.com/library/llava-phi3:3.8b",
        "parameters": "3.8b",
        "pull_count": "84.6K",
        "tag_count": "4",
        "updated": "10 months ago",
        "updated_time": 1717654968,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "nous-hermes:7b",
        "name": "nous-hermes",
        "msg": "General use models based on Llama and Llama 2 from Nous Research.",
        "size": "3.8GB",
        "zh_cn_msg": "基于Nous Research的Llama和Llama 2的一般用途模型。",
        "link": "https://ollama.com/library/nous-hermes:7b",
        "parameters": "7b",
        "pull_count": "83.5K",
        "tag_count": "63",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nous-hermes:13b",
        "name": "nous-hermes",
        "msg": "General use models based on Llama and Llama 2 from Nous Research.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Nous Research的Llama和Llama 2的一般用途模型。",
        "link": "https://ollama.com/library/nous-hermes:13b",
        "parameters": "13b",
        "pull_count": "83.5K",
        "tag_count": "63",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "phind-codellama:34b",
        "name": "phind-codellama",
        "msg": "Code generation model based on Code Llama.",
        "size": "19GB",
        "zh_cn_msg": "基于Code Llama的代码生成模型。",
        "link": "https://ollama.com/library/phind-codellama:34b",
        "parameters": "34b",
        "pull_count": "82.4K",
        "tag_count": "49",
        "updated": "15 months ago",
        "updated_time": 1704694968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "starling-lm:7b",
        "name": "starling-lm",
        "msg": "Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.",
        "size": "4.1GB",
        "zh_cn_msg": "Starling 是一个通过从 AI 反馈中进行强化学习训练的大语言模型，其重点是提高聊天机器人的有用性。",
        "link": "https://ollama.com/library/starling-lm:7b",
        "parameters": "7b",
        "pull_count": "82.2K",
        "tag_count": "36",
        "updated": "12 months ago",
        "updated_time": 1712470968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "solar:10.7b",
        "name": "solar",
        "msg": "A compact, yet powerful 10.7B large language model designed for single-turn conversation.",
        "size": "6.1GB",
        "zh_cn_msg": "一个精简而强大的10.7B参数的大规模语言模型，专为单轮对话设计。",
        "link": "https://ollama.com/library/solar:10.7b",
        "parameters": "10.7b",
        "pull_count": "80.1K",
        "tag_count": "32",
        "updated": "15 months ago",
        "updated_time": 1704694968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yarn-llama2:7b",
        "name": "yarn-llama2",
        "msg": "An extension of Llama 2 that supports a context of up to 128k tokens.",
        "size": "3.8GB",
        "zh_cn_msg": "Llama 2的一个扩展版本，支持多达128k令牌的上下文。",
        "link": "https://ollama.com/library/yarn-llama2:7b",
        "parameters": "7b",
        "pull_count": "79.6K",
        "tag_count": "67",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yarn-llama2:13b",
        "name": "yarn-llama2",
        "msg": "An extension of Llama 2 that supports a context of up to 128k tokens.",
        "size": "7.4GB",
        "zh_cn_msg": "Llama 2的一个扩展版本，支持多达128k令牌的上下文。",
        "link": "https://ollama.com/library/yarn-llama2:13b",
        "parameters": "13b",
        "pull_count": "79.6K",
        "tag_count": "67",
        "updated": "17 months ago",
        "updated_time": 1699510968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yi-coder:1.5b",
        "name": "yi-coder",
        "msg": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
        "size": "866MB",
        "zh_cn_msg": "Yi-Coder 是一系列开源代码语言模型，使用少于10亿参数即可实现最先进的编码性能。",
        "link": "https://ollama.com/library/yi-coder:1.5b",
        "parameters": "1.5b",
        "pull_count": "79.1K",
        "tag_count": "67",
        "updated": "6 months ago",
        "updated_time": 1728022968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yi-coder:9b",
        "name": "yi-coder",
        "msg": "Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.",
        "size": "5.0GB",
        "zh_cn_msg": "Yi-Coder 是一系列开源代码语言模型，使用少于10亿参数即可实现最先进的编码性能。",
        "link": "https://ollama.com/library/yi-coder:9b",
        "parameters": "9b",
        "pull_count": "79.1K",
        "tag_count": "67",
        "updated": "6 months ago",
        "updated_time": 1728022968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "athene-v2:72b",
        "name": "athene-v2",
        "msg": "Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.",
        "size": "47GB",
        "zh_cn_msg": "Athene-V2 是一个拥有720亿参数的模型，在代码完成、数学和日志提取任务方面表现出色。",
        "link": "https://ollama.com/library/athene-v2:72b",
        "parameters": "72b",
        "pull_count": "78K",
        "tag_count": "17",
        "updated": "4 months ago",
        "updated_time": 1733206968,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "internlm2:1m",
        "name": "internlm2",
        "msg": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
        "size": "4.5GB",
        "zh_cn_msg": "InternLM2.5 是一个针对实际场景优化的70亿参数模型，具备出色的推理能力。",
        "link": "https://ollama.com/library/internlm2:1m",
        "parameters": "1m",
        "pull_count": "75.9K",
        "tag_count": "65",
        "updated": "7 months ago",
        "updated_time": 1725430968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "internlm2:1.8b",
        "name": "internlm2",
        "msg": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
        "size": "1.1GB",
        "zh_cn_msg": "InternLM2.5 是一个针对实际场景优化的70亿参数模型，具备出色的推理能力。",
        "link": "https://ollama.com/library/internlm2:1.8b",
        "parameters": "1.8b",
        "pull_count": "75.9K",
        "tag_count": "65",
        "updated": "7 months ago",
        "updated_time": 1725430968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "internlm2:7b",
        "name": "internlm2",
        "msg": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
        "size": "4.5GB",
        "zh_cn_msg": "InternLM2.5 是一个针对实际场景优化的70亿参数模型，具备出色的推理能力。",
        "link": "https://ollama.com/library/internlm2:7b",
        "parameters": "7b",
        "pull_count": "75.9K",
        "tag_count": "65",
        "updated": "7 months ago",
        "updated_time": 1725430968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "internlm2:20b",
        "name": "internlm2",
        "msg": "InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.",
        "size": "11GB",
        "zh_cn_msg": "InternLM2.5 是一个针对实际场景优化的70亿参数模型，具备出色的推理能力。",
        "link": "https://ollama.com/library/internlm2:20b",
        "parameters": "20b",
        "pull_count": "75.9K",
        "tag_count": "65",
        "updated": "7 months ago",
        "updated_time": 1725430968,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nemotron-mini:4b",
        "name": "nemotron-mini",
        "msg": "A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.",
        "size": "2.7GB",
        "zh_cn_msg": "NVIDIA推出的一款面向商业友好的小型语言模型，经过优化适用于角色扮演、检索增强生成问答和函数调用。",
        "link": "https://ollama.com/library/nemotron-mini:4b",
        "parameters": "4b",
        "pull_count": "73.3K",
        "tag_count": "17",
        "updated": "6 months ago",
        "updated_time": 1728022969,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "falcon:7b",
        "name": "falcon",
        "msg": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
        "size": "4.2GB",
        "zh_cn_msg": "由技术创新研究所（TII）构建的一个大型语言模型，用于摘要生成、文本生成和聊天机器人。",
        "link": "https://ollama.com/library/falcon:7b",
        "parameters": "7b",
        "pull_count": "71.5K",
        "tag_count": "38",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon:40b",
        "name": "falcon",
        "msg": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
        "size": "24GB",
        "zh_cn_msg": "由技术创新研究所（TII）构建的一个大型语言模型，用于摘要生成、文本生成和聊天机器人。",
        "link": "https://ollama.com/library/falcon:40b",
        "parameters": "40b",
        "pull_count": "71.5K",
        "tag_count": "38",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon:180b",
        "name": "falcon",
        "msg": "A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.",
        "size": "101GB",
        "zh_cn_msg": "由技术创新研究所（TII）构建的一个大型语言模型，用于摘要生成、文本生成和聊天机器人。",
        "link": "https://ollama.com/library/falcon:180b",
        "parameters": "180b",
        "pull_count": "71.5K",
        "tag_count": "38",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "deepscaler:1.5b",
        "name": "deepscaler",
        "msg": "A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses the performance of OpenAI’s o1-preview with just 1.5B parameters on popular math evaluations.",
        "size": "3.6GB",
        "zh_cn_msg": "Deepseek-R1-Distilled-Qwen-1.5B的精调版本，它在流行的数学评估中仅使用1.5B参数就超越了OpenAI的o1-preview的表现。",
        "link": "https://ollama.com/library/deepscaler:1.5b",
        "parameters": "1.5b",
        "pull_count": "71.5K",
        "tag_count": "5",
        "updated": "7 weeks ago",
        "updated_time": 1739341369,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nemotron:70b",
        "name": "nemotron",
        "msg": "Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.",
        "size": "43GB",
        "zh_cn_msg": "NVIDIA对Llama-3.1-Nemotron-70B-Instruct这款大型语言模型进行了定制，以提高其生成的用户查询回复的有用性。",
        "link": "https://ollama.com/library/nemotron:70b",
        "parameters": "70b",
        "pull_count": "69.3K",
        "tag_count": "17",
        "updated": "5 months ago",
        "updated_time": 1730614969,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "dolphin-phi:2.7b",
        "name": "dolphin-phi",
        "msg": "2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.",
        "size": "1.6GB",
        "zh_cn_msg": "27亿参数未经过滤的Dolphin模型，由Eric Hartford基于Microsoft Research的Phi语言模型创建。",
        "link": "https://ollama.com/library/dolphin-phi:2.7b",
        "parameters": "2.7b",
        "pull_count": "68.4K",
        "tag_count": "15",
        "updated": "15 months ago",
        "updated_time": 1704694969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3-dense:2b",
        "name": "granite3-dense",
        "msg": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
        "size": "1.6GB",
        "zh_cn_msg": "IBM Granite 2B 和 8B 模型设计用于支持基于工具的用例和检索增强生成（RAG），简化代码生成、翻译和修复错误。",
        "link": "https://ollama.com/library/granite3-dense:2b",
        "parameters": "2b",
        "pull_count": "66.8K",
        "tag_count": "33",
        "updated": "4 months ago",
        "updated_time": 1733206969,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3-dense:8b",
        "name": "granite3-dense",
        "msg": "The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.",
        "size": "4.9GB",
        "zh_cn_msg": "IBM Granite 2B 和 8B 模型设计用于支持基于工具的用例和检索增强生成（RAG），简化代码生成、翻译和修复错误。",
        "link": "https://ollama.com/library/granite3-dense:8b",
        "parameters": "8b",
        "pull_count": "66.8K",
        "tag_count": "33",
        "updated": "4 months ago",
        "updated_time": 1733206969,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "orca2:7b",
        "name": "orca2",
        "msg": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.",
        "size": "3.8GB",
        "zh_cn_msg": "Orca 2 是由微软研究团队构建的，它是Meta的Llama 2模型的精调版本。该模型在推理方面表现尤为出色。",
        "link": "https://ollama.com/library/orca2:7b",
        "parameters": "7b",
        "pull_count": "64.6K",
        "tag_count": "33",
        "updated": "16 months ago",
        "updated_time": 1702102969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "orca2:13b",
        "name": "orca2",
        "msg": "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models.  The model is designed to excel particularly in reasoning.",
        "size": "7.4GB",
        "zh_cn_msg": "Orca 2 是由微软研究团队构建的，它是Meta的Llama 2模型的精调版本。该模型在推理方面表现尤为出色。",
        "link": "https://ollama.com/library/orca2:13b",
        "parameters": "13b",
        "pull_count": "64.6K",
        "tag_count": "33",
        "updated": "16 months ago",
        "updated_time": 1702102969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizardlm-uncensored:13b",
        "name": "wizardlm-uncensored",
        "msg": "Uncensored version of Wizard LM model ",
        "size": "7.4GB",
        "zh_cn_msg": "未经过滤版本的Wizard LM模型",
        "link": "https://ollama.com/library/wizardlm-uncensored:13b",
        "parameters": "13b",
        "pull_count": "62.4K",
        "tag_count": "18",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "stable-beluga:7b",
        "name": "stable-beluga",
        "msg": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
        "size": "3.8GB",
        "zh_cn_msg": "基于Llama 2的模型，在类似Orca的数据集上进行了微调。最初被称为Free Willy。",
        "link": "https://ollama.com/library/stable-beluga:7b",
        "parameters": "7b",
        "pull_count": "59.6K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "stable-beluga:13b",
        "name": "stable-beluga",
        "msg": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Llama 2的模型，在类似Orca的数据集上进行了微调。最初被称为Free Willy。",
        "link": "https://ollama.com/library/stable-beluga:13b",
        "parameters": "13b",
        "pull_count": "59.6K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "stable-beluga:70b",
        "name": "stable-beluga",
        "msg": "Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.",
        "size": "39GB",
        "zh_cn_msg": "基于Llama 2的模型，在类似Orca的数据集上进行了微调。最初被称为Free Willy。",
        "link": "https://ollama.com/library/stable-beluga:70b",
        "parameters": "70b",
        "pull_count": "59.6K",
        "tag_count": "49",
        "updated": "17 months ago",
        "updated_time": 1699510969,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama3-groq-tool-use:8b",
        "name": "llama3-groq-tool-use",
        "msg": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.",
        "size": "4.7GB",
        "zh_cn_msg": "Groq的一系列模型，代表了开源人工智能工具使用/函数调用能力的重大进步。",
        "link": "https://ollama.com/library/llama3-groq-tool-use:8b",
        "parameters": "8b",
        "pull_count": "59.2K",
        "tag_count": "33",
        "updated": "8 months ago",
        "updated_time": 1722838970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "llama3-groq-tool-use:70b",
        "name": "llama3-groq-tool-use",
        "msg": "A series of models from Groq that represent a significant advancement in open-source AI capabilities for tool use/function calling.",
        "size": "40GB",
        "zh_cn_msg": "Groq的一系列模型，代表了开源人工智能工具使用/函数调用能力的重大进步。",
        "link": "https://ollama.com/library/llama3-groq-tool-use:70b",
        "parameters": "70b",
        "pull_count": "59.2K",
        "tag_count": "33",
        "updated": "8 months ago",
        "updated_time": 1722838970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "paraphrase-multilingual:278m",
        "name": "paraphrase-multilingual",
        "msg": "Sentence-transformers model that can be used for tasks like clustering or semantic search.",
        "size": "563MB",
        "zh_cn_msg": "可以用于聚类或语义搜索等任务的Sentence-transformers模型。",
        "link": "https://ollama.com/library/paraphrase-multilingual:278m",
        "parameters": "278m",
        "pull_count": "56K",
        "tag_count": "3",
        "updated": "7 months ago",
        "updated_time": 1725430970,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "granite3.2:2b",
        "name": "granite3.2",
        "msg": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
        "size": "1.5GB",
        "zh_cn_msg": "Granite-3.2 是IBM Granite系列中经过微调、具备强大思维能力的长上下文AI模型家族。",
        "link": "https://ollama.com/library/granite3.2:2b",
        "parameters": "2b",
        "pull_count": "54.6K",
        "tag_count": "9",
        "updated": "5 weeks ago",
        "updated_time": 1740550970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3.2:8b",
        "name": "granite3.2",
        "msg": "Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.",
        "size": "4.9GB",
        "zh_cn_msg": "Granite-3.2 是IBM Granite系列中经过微调、具备强大思维能力的长上下文AI模型家族。",
        "link": "https://ollama.com/library/granite3.2:8b",
        "parameters": "8b",
        "pull_count": "54.6K",
        "tag_count": "9",
        "updated": "5 weeks ago",
        "updated_time": 1740550970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "snowflake-arctic-embed2:568m",
        "name": "snowflake-arctic-embed2",
        "msg": "Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support without sacrificing English performance or scalability.",
        "size": "1.2GB",
        "zh_cn_msg": "雪 Flake 的前沿嵌入模型。Arctic Embed 2.0 增加了多语言支持，同时不牺牲英语性能或可扩展性。",
        "link": "https://ollama.com/library/snowflake-arctic-embed2:568m",
        "parameters": "568m",
        "pull_count": "53.7K",
        "tag_count": "3",
        "updated": "3 months ago",
        "updated_time": 1735798970,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "deepseek-v2.5:236b",
        "name": "deepseek-v2.5",
        "msg": "An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.",
        "size": "133GB",
        "zh_cn_msg": "DeekSeek-V2 的升级版本，集成了 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct 的通用和编码能力。",
        "link": "https://ollama.com/library/deepseek-v2.5:236b",
        "parameters": "236b",
        "pull_count": "51.3K",
        "tag_count": "7",
        "updated": "6 months ago",
        "updated_time": 1728022970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "smallthinker:3b",
        "name": "smallthinker",
        "msg": "A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.",
        "size": "3.6GB",
        "zh_cn_msg": "一个新的从小Qwen 2.5 3B Instruct模型微调而来的推理模型。",
        "link": "https://ollama.com/library/smallthinker:3b",
        "parameters": "3b",
        "pull_count": "51K",
        "tag_count": "5",
        "updated": "3 months ago",
        "updated_time": 1735798970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "meditron:7b",
        "name": "meditron",
        "msg": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
        "size": "3.8GB",
        "zh_cn_msg": "开源的医学大型语言模型，基于Llama 2适应于医疗领域。",
        "link": "https://ollama.com/library/meditron:7b",
        "parameters": "7b",
        "pull_count": "49.4K",
        "tag_count": "22",
        "updated": "16 months ago",
        "updated_time": 1702102970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "meditron:70b",
        "name": "meditron",
        "msg": "Open-source medical large language model adapted from Llama 2 to the medical domain.",
        "size": "39GB",
        "zh_cn_msg": "开源的医学大型语言模型，基于Llama 2适应于医疗领域。",
        "link": "https://ollama.com/library/meditron:70b",
        "parameters": "70b",
        "pull_count": "49.4K",
        "tag_count": "22",
        "updated": "16 months ago",
        "updated_time": 1702102970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "aya-expanse:8b",
        "name": "aya-expanse",
        "msg": "Cohere For AI's language models trained to perform well across 23 different languages.",
        "size": "5.1GB",
        "zh_cn_msg": "Cohere for AI的语言模型经过训练，在23种不同的语言中表现出色。",
        "link": "https://ollama.com/library/aya-expanse:8b",
        "parameters": "8b",
        "pull_count": "49.4K",
        "tag_count": "33",
        "updated": "5 months ago",
        "updated_time": 1730614970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "aya-expanse:32b",
        "name": "aya-expanse",
        "msg": "Cohere For AI's language models trained to perform well across 23 different languages.",
        "size": "20GB",
        "zh_cn_msg": "Cohere for AI的语言模型经过训练，在23种不同的语言中表现出色。",
        "link": "https://ollama.com/library/aya-expanse:32b",
        "parameters": "32b",
        "pull_count": "49.4K",
        "tag_count": "33",
        "updated": "5 months ago",
        "updated_time": 1730614970,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "medllama2:7b",
        "name": "medllama2",
        "msg": "Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset. ",
        "size": "3.8GB",
        "zh_cn_msg": "基于开源医疗数据集对Llama 2模型进行了微调，以回答医学问题。",
        "link": "https://ollama.com/library/medllama2:7b",
        "parameters": "7b",
        "pull_count": "48.9K",
        "tag_count": "17",
        "updated": "17 months ago",
        "updated_time": 1699510970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama-pro:latest",
        "name": "llama-pro",
        "msg": "An expansion of Llama 2 that specializes in integrating both general language understanding and domain-specific knowledge, particularly in programming and mathematics.",
        "size": "4.7GB",
        "zh_cn_msg": "Llama 2的一个扩展版本，专注于融合通用语言理解和特定领域的知识，特别是在编程和数学领域。",
        "link": "https://ollama.com/library/llama-pro:latest",
        "parameters": "latest",
        "pull_count": "46.3K",
        "tag_count": "33",
        "updated": "14 months ago",
        "updated_time": 1707286970,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3-moe:1b",
        "name": "granite3-moe",
        "msg": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
        "size": "822MB",
        "zh_cn_msg": "IBM 的 Granite 1B 和 3B 模型是 IBM 设计的第一批混合专家（MoE）Granite 模型，专为低延迟使用而设计。",
        "link": "https://ollama.com/library/granite3-moe:1b",
        "parameters": "1b",
        "pull_count": "46.2K",
        "tag_count": "33",
        "updated": "4 months ago",
        "updated_time": 1733206971,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3-moe:3b",
        "name": "granite3-moe",
        "msg": "The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
        "size": "2.1GB",
        "zh_cn_msg": "IBM 的 Granite 1B 和 3B 模型是 IBM 设计的第一批混合专家（MoE）Granite 模型，专为低延迟使用而设计。",
        "link": "https://ollama.com/library/granite3-moe:3b",
        "parameters": "3b",
        "pull_count": "46.2K",
        "tag_count": "33",
        "updated": "4 months ago",
        "updated_time": 1733206971,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "falcon3:1b",
        "name": "falcon3",
        "msg": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
        "size": "1.8GB",
        "zh_cn_msg": "在科学、数学和编码方面通过创新训练技术表现优秀的低于100亿参数的高效AI模型系列。",
        "link": "https://ollama.com/library/falcon3:1b",
        "parameters": "1b",
        "pull_count": "45.9K",
        "tag_count": "17",
        "updated": "3 months ago",
        "updated_time": 1735798971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon3:3b",
        "name": "falcon3",
        "msg": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
        "size": "2.0GB",
        "zh_cn_msg": "在科学、数学和编码方面通过创新训练技术表现优秀的低于100亿参数的高效AI模型系列。",
        "link": "https://ollama.com/library/falcon3:3b",
        "parameters": "3b",
        "pull_count": "45.9K",
        "tag_count": "17",
        "updated": "3 months ago",
        "updated_time": 1735798971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon3:7b",
        "name": "falcon3",
        "msg": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
        "size": "4.6GB",
        "zh_cn_msg": "在科学、数学和编码方面通过创新训练技术表现优秀的低于100亿参数的高效AI模型系列。",
        "link": "https://ollama.com/library/falcon3:7b",
        "parameters": "7b",
        "pull_count": "45.9K",
        "tag_count": "17",
        "updated": "3 months ago",
        "updated_time": 1735798971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon3:10b",
        "name": "falcon3",
        "msg": "A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",
        "size": "6.3GB",
        "zh_cn_msg": "在科学、数学和编码方面通过创新训练技术表现优秀的低于100亿参数的高效AI模型系列。",
        "link": "https://ollama.com/library/falcon3:10b",
        "parameters": "10b",
        "pull_count": "45.9K",
        "tag_count": "17",
        "updated": "3 months ago",
        "updated_time": 1735798971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "yarn-mistral:7b",
        "name": "yarn-mistral",
        "msg": "An extension of Mistral to support context windows of 64K or 128K.",
        "size": "4.1GB",
        "zh_cn_msg": "将Mistral扩展以支持64K或128K的上下文窗口。",
        "link": "https://ollama.com/library/yarn-mistral:7b",
        "parameters": "7b",
        "pull_count": "45.7K",
        "tag_count": "33",
        "updated": "17 months ago",
        "updated_time": 1699510971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nexusraven:13b",
        "name": "nexusraven",
        "msg": "Nexus Raven is a 13B instruction tuned model for function calling tasks. ",
        "size": "7.4GB",
        "zh_cn_msg": "Nexus Raven 是一个经过微调用于函数调用任务的130亿参数指令模型。",
        "link": "https://ollama.com/library/nexusraven:13b",
        "parameters": "13b",
        "pull_count": "42.2K",
        "tag_count": "32",
        "updated": "14 months ago",
        "updated_time": 1707286971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codeup:13b",
        "name": "codeup",
        "msg": "Great code generation model based on Llama2.",
        "size": "7.4GB",
        "zh_cn_msg": "基于Llama2的强大代码生成模型。",
        "link": "https://ollama.com/library/codeup:13b",
        "parameters": "13b",
        "pull_count": "40.2K",
        "tag_count": "19",
        "updated": "17 months ago",
        "updated_time": 1699510971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "everythinglm:13b",
        "name": "everythinglm",
        "msg": "Uncensored Llama2 based model with support for a 16K context window.",
        "size": "7.4GB",
        "zh_cn_msg": "未经过滤的Llama2模型，支持16K上下文窗口。",
        "link": "https://ollama.com/library/everythinglm:13b",
        "parameters": "13b",
        "pull_count": "39.3K",
        "tag_count": "18",
        "updated": "15 months ago",
        "updated_time": 1704694971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nous-hermes2-mixtral:8x7b",
        "name": "nous-hermes2-mixtral",
        "msg": "The Nous Hermes 2 model from Nous Research, now trained over Mixtral.",
        "size": "26GB",
        "zh_cn_msg": "Nous Research 的 Nous Hermes 2 模型现已基于 Mixtral 进行训练。",
        "link": "https://ollama.com/library/nous-hermes2-mixtral:8x7b",
        "parameters": "8x7b",
        "pull_count": "39K",
        "tag_count": "18",
        "updated": "3 months ago",
        "updated_time": 1735798971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3.1-moe:1b",
        "name": "granite3.1-moe",
        "msg": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
        "size": "1.4GB",
        "zh_cn_msg": "IBM的Granite 1B和3B模型是IBM设计的长上下文专家混合（MoE）Granite模型，适用于低延迟使用。",
        "link": "https://ollama.com/library/granite3.1-moe:1b",
        "parameters": "1b",
        "pull_count": "38.3K",
        "tag_count": "33",
        "updated": "2 months ago",
        "updated_time": 1738390971,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "granite3.1-moe:3b",
        "name": "granite3.1-moe",
        "msg": "The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.",
        "size": "2.0GB",
        "zh_cn_msg": "IBM的Granite 1B和3B模型是IBM设计的长上下文专家混合（MoE）Granite模型，适用于低延迟使用。",
        "link": "https://ollama.com/library/granite3.1-moe:3b",
        "parameters": "3b",
        "pull_count": "38.3K",
        "tag_count": "33",
        "updated": "2 months ago",
        "updated_time": 1738390971,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "shieldgemma:2b",
        "name": "shieldgemma",
        "msg": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
        "size": "1.7GB",
        "zh_cn_msg": "ShieldGemma 是一组经过调整的指令模型，用于评估文本提示输入和文本输出响应是否符合定义的安全政策。",
        "link": "https://ollama.com/library/shieldgemma:2b",
        "parameters": "2b",
        "pull_count": "37.9K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "shieldgemma:9b",
        "name": "shieldgemma",
        "msg": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
        "size": "5.8GB",
        "zh_cn_msg": "ShieldGemma 是一组经过调整的指令模型，用于评估文本提示输入和文本输出响应是否符合定义的安全政策。",
        "link": "https://ollama.com/library/shieldgemma:9b",
        "parameters": "9b",
        "pull_count": "37.9K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "shieldgemma:27b",
        "name": "shieldgemma",
        "msg": "ShieldGemma is set of instruction tuned models for evaluating the safety of text prompt input and text output responses against a set of defined safety policies.",
        "size": "17GB",
        "zh_cn_msg": "ShieldGemma 是一组经过调整的指令模型，用于评估文本提示输入和文本输出响应是否符合定义的安全政策。",
        "link": "https://ollama.com/library/shieldgemma:27b",
        "parameters": "27b",
        "pull_count": "37.9K",
        "tag_count": "49",
        "updated": "5 months ago",
        "updated_time": 1730614971,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "reader-lm:0.5b",
        "name": "reader-lm",
        "msg": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
        "size": "352MB",
        "zh_cn_msg": "一系列将HTML内容转换为Markdown内容的模型，对于内容转换任务非常有用。",
        "link": "https://ollama.com/library/reader-lm:0.5b",
        "parameters": "0.5b",
        "pull_count": "34.8K",
        "tag_count": "33",
        "updated": "6 months ago",
        "updated_time": 1728022972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "reader-lm:1.5b",
        "name": "reader-lm",
        "msg": "A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.",
        "size": "935MB",
        "zh_cn_msg": "一系列将HTML内容转换为Markdown内容的模型，对于内容转换任务非常有用。",
        "link": "https://ollama.com/library/reader-lm:1.5b",
        "parameters": "1.5b",
        "pull_count": "34.8K",
        "tag_count": "33",
        "updated": "6 months ago",
        "updated_time": 1728022972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "marco-o1:7b",
        "name": "marco-o1",
        "msg": "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
        "size": "4.7GB",
        "zh_cn_msg": "阿里国际数字商业集团（AIDC-AI）的一个开源大型推理模型，用于现实世界解决方案。",
        "link": "https://ollama.com/library/marco-o1:7b",
        "parameters": "7b",
        "pull_count": "34.4K",
        "tag_count": "5",
        "updated": "3 months ago",
        "updated_time": 1735798972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mathstral:7b",
        "name": "mathstral",
        "msg": "MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.",
        "size": "4.1GB",
        "zh_cn_msg": "MathΣtral：由Mistral AI 设计的一款专用于数学推理和科学发现的70亿参数模型。",
        "link": "https://ollama.com/library/mathstral:7b",
        "parameters": "7b",
        "pull_count": "34.1K",
        "tag_count": "17",
        "updated": "8 months ago",
        "updated_time": 1722838972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone3.5:2.4b",
        "name": "exaone3.5",
        "msg": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. ",
        "size": "1.6GB",
        "zh_cn_msg": "EXAONE 3.5 是由LG人工智能研究所开发并发布的一系列指令微调的双语（英语和韩语）生成模型，参数规模从2.4B到32B不等。",
        "link": "https://ollama.com/library/exaone3.5:2.4b",
        "parameters": "2.4b",
        "pull_count": "33.8K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone3.5:7.8b",
        "name": "exaone3.5",
        "msg": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. ",
        "size": "4.8GB",
        "zh_cn_msg": "EXAONE 3.5 是由LG人工智能研究所开发并发布的一系列指令微调的双语（英语和韩语）生成模型，参数规模从2.4B到32B不等。",
        "link": "https://ollama.com/library/exaone3.5:7.8b",
        "parameters": "7.8b",
        "pull_count": "33.8K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone3.5:32b",
        "name": "exaone3.5",
        "msg": "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. ",
        "size": "19GB",
        "zh_cn_msg": "EXAONE 3.5 是由LG人工智能研究所开发并发布的一系列指令微调的双语（英语和韩语）生成模型，参数规模从2.4B到32B不等。",
        "link": "https://ollama.com/library/exaone3.5:32b",
        "parameters": "32b",
        "pull_count": "33.8K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama-guard3:1b",
        "name": "llama-guard3",
        "msg": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
        "size": "1.6GB",
        "zh_cn_msg": "Llama Guard 3 是一系列经过微调的模型，用于对大规模语言模型的输入和响应进行内容安全分类。",
        "link": "https://ollama.com/library/llama-guard3:1b",
        "parameters": "1b",
        "pull_count": "33.7K",
        "tag_count": "33",
        "updated": "5 months ago",
        "updated_time": 1730614972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "llama-guard3:8b",
        "name": "llama-guard3",
        "msg": "Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.",
        "size": "4.9GB",
        "zh_cn_msg": "Llama Guard 3 是一系列经过微调的模型，用于对大规模语言模型的输入和响应进行内容安全分类。",
        "link": "https://ollama.com/library/llama-guard3:8b",
        "parameters": "8b",
        "pull_count": "33.7K",
        "tag_count": "33",
        "updated": "5 months ago",
        "updated_time": 1730614972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "solar-pro:22b",
        "name": "solar-pro",
        "msg": "Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU",
        "size": "13GB",
        "zh_cn_msg": "Solar Pro 预览版：一个先进的大型语言模型（LLM），拥有 220 亿个参数，设计用于单个 GPU 上运行。",
        "link": "https://ollama.com/library/solar-pro:22b",
        "parameters": "22b",
        "pull_count": "33.5K",
        "tag_count": "18",
        "updated": "6 months ago",
        "updated_time": 1728022972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "falcon2:11b",
        "name": "falcon2",
        "msg": "Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.",
        "size": "6.4GB",
        "zh_cn_msg": "Falcon2 是由 TII 创建的一个拥有 110 亿参数的因果解码器-only 模型，经过 5T 令牌训练。",
        "link": "https://ollama.com/library/falcon2:11b",
        "parameters": "11b",
        "pull_count": "33.2K",
        "tag_count": "17",
        "updated": "10 months ago",
        "updated_time": 1717654972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3.2-vision:2b",
        "name": "granite3.2-vision",
        "msg": "A compact and efficient vision-language model, specifically designed for visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, and more.",
        "size": "2.4GB",
        "zh_cn_msg": "一个紧凑而高效的视觉-语言模型，专门设计用于视觉文档理解，能够从表格、图表、信息图、图形、图表等自动提取内容。",
        "link": "https://ollama.com/library/granite3.2-vision:2b",
        "parameters": "2b",
        "pull_count": "32.9K",
        "tag_count": "5",
        "updated": "4 weeks ago",
        "updated_time": 1741155772,
        "capability": [
            "llm",
            "vision"
        ]
    },
    {
        "full_name": "stablelm-zephyr:3b",
        "name": "stablelm-zephyr",
        "msg": "A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.",
        "size": "1.6GB",
        "zh_cn_msg": "一个轻量级的聊天模型，能够在不依赖高端硬件的情况下提供准确且响应迅速的输出。",
        "link": "https://ollama.com/library/stablelm-zephyr:3b",
        "parameters": "3b",
        "pull_count": "32.8K",
        "tag_count": "17",
        "updated": "15 months ago",
        "updated_time": 1704694972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "magicoder:7b",
        "name": "magicoder",
        "msg": "🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets.",
        "size": "3.8GB",
        "zh_cn_msg": "🎩 Magicoder 是一组参数为70亿的模型，它们是在7.5万个合成指令数据上使用OSS-Instruct训练的。OSS-Instruct是一种新颖的方法，通过开源代码片段来启发大规模语言模型。",
        "link": "https://ollama.com/library/magicoder:7b",
        "parameters": "7b",
        "pull_count": "32.6K",
        "tag_count": "18",
        "updated": "16 months ago",
        "updated_time": 1702102972,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "codebooga:34b",
        "name": "codebooga",
        "msg": "A high-performing code instruct model created by merging two existing code models.",
        "size": "19GB",
        "zh_cn_msg": "一个高性能的代码指令模型，通过合并两个现有的代码模型创建而成。",
        "link": "https://ollama.com/library/codebooga:34b",
        "parameters": "34b",
        "pull_count": "31.8K",
        "tag_count": "16",
        "updated": "17 months ago",
        "updated_time": 1699510973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "duckdb-nsql:7b",
        "name": "duckdb-nsql",
        "msg": "7B parameter text-to-SQL model made by MotherDuck and Numbers Station.",
        "size": "3.8GB",
        "zh_cn_msg": "由MotherDuck和Numbers Station开发的70亿参数文本转SQL模型。",
        "link": "https://ollama.com/library/duckdb-nsql:7b",
        "parameters": "7b",
        "pull_count": "31.4K",
        "tag_count": "17",
        "updated": "14 months ago",
        "updated_time": 1707286973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "mistrallite:7b",
        "name": "mistrallite",
        "msg": "MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.",
        "size": "4.1GB",
        "zh_cn_msg": "MistralLite 是基于 Mistral 的一个经过微调的模型，具备处理长上下文的增强能力。",
        "link": "https://ollama.com/library/mistrallite:7b",
        "parameters": "7b",
        "pull_count": "30.7K",
        "tag_count": "17",
        "updated": "17 months ago",
        "updated_time": 1699510973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "wizard-vicuna:13b",
        "name": "wizard-vicuna",
        "msg": "Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.",
        "size": "7.4GB",
        "zh_cn_msg": "Wizard Vicuna 是由 MelodysDreamj 基于 Llama 2 训练的一个 130 亿参数的模型。",
        "link": "https://ollama.com/library/wizard-vicuna:13b",
        "parameters": "13b",
        "pull_count": "29.9K",
        "tag_count": "17",
        "updated": "17 months ago",
        "updated_time": 1699510973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "command-r7b:7b",
        "name": "command-r7b",
        "msg": "The smallest model in Cohere's R series delivers top-tier speed, efficiency, and quality to build powerful AI applications on commodity GPUs and edge devices.",
        "size": "5.1GB",
        "zh_cn_msg": "Cohere 的 R 系列中最小的模型能够在通用 GPU 和边缘设备上提供顶级的速度、效率和质量，以构建强大的人工智能应用程序。",
        "link": "https://ollama.com/library/command-r7b:7b",
        "parameters": "7b",
        "pull_count": "28.3K",
        "tag_count": "5",
        "updated": "2 months ago",
        "updated_time": 1738390973,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "opencoder:1.5b",
        "name": "opencoder",
        "msg": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.",
        "size": "1.4GB",
        "zh_cn_msg": "OpenCoder 是一个开放且可重复再现的代码大模型家族，其中包括1.5B和8B参数的模型，并支持英语和中文的对话。",
        "link": "https://ollama.com/library/opencoder:1.5b",
        "parameters": "1.5b",
        "pull_count": "28K",
        "tag_count": "9",
        "updated": "4 months ago",
        "updated_time": 1733206973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "opencoder:8b",
        "name": "opencoder",
        "msg": "OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting chat in English and Chinese languages.",
        "size": "4.7GB",
        "zh_cn_msg": "OpenCoder 是一个开放且可重复再现的代码大模型家族，其中包括1.5B和8B参数的模型，并支持英语和中文的对话。",
        "link": "https://ollama.com/library/opencoder:8b",
        "parameters": "8b",
        "pull_count": "28K",
        "tag_count": "9",
        "updated": "4 months ago",
        "updated_time": 1733206973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "nuextract:3.8b",
        "name": "nuextract",
        "msg": "A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.",
        "size": "2.2GB",
        "zh_cn_msg": "一个基于Phi-3、在高质量合成数据集上进行微调的38亿参数模型，用于信息提取。",
        "link": "https://ollama.com/library/nuextract:3.8b",
        "parameters": "3.8b",
        "pull_count": "27.6K",
        "tag_count": "17",
        "updated": "8 months ago",
        "updated_time": 1722838973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite-embedding:30m",
        "name": "granite-embedding",
        "msg": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.",
        "size": "63MB",
        "zh_cn_msg": "IBM Granite Embedding 30M 和 278M 模型是仅处理文本的密集双编码嵌入模型，其中 30M 模型仅提供英文版本，而 278M 模型支持多语言应用场景。",
        "link": "https://ollama.com/library/granite-embedding:30m",
        "parameters": "30m",
        "pull_count": "27.5K",
        "tag_count": "6",
        "updated": "3 months ago",
        "updated_time": 1735798973,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "granite-embedding:278m",
        "name": "granite-embedding",
        "msg": "The IBM Granite Embedding 30M and 278M models models are text-only dense biencoder embedding models, with 30M available in English only and 278M serving multilingual use cases.",
        "size": "563MB",
        "zh_cn_msg": "IBM Granite Embedding 30M 和 278M 模型是仅处理文本的密集双编码嵌入模型，其中 30M 模型仅提供英文版本，而 278M 模型支持多语言应用场景。",
        "link": "https://ollama.com/library/granite-embedding:278m",
        "parameters": "278m",
        "pull_count": "27.5K",
        "tag_count": "6",
        "updated": "3 months ago",
        "updated_time": 1735798973,
        "capability": [
            "embedding"
        ]
    },
    {
        "full_name": "megadolphin:120b",
        "name": "megadolphin",
        "msg": "MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.",
        "size": "68GB",
        "zh_cn_msg": "MegaDolphin-2.2-120b 是通过将其与自身交错融合而创建的 Dolphin-2.2-70b 的改进版本。",
        "link": "https://ollama.com/library/megadolphin:120b",
        "parameters": "120b",
        "pull_count": "25.5K",
        "tag_count": "19",
        "updated": "14 months ago",
        "updated_time": 1707286973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "bespoke-minicheck:7b",
        "name": "bespoke-minicheck",
        "msg": "A state-of-the-art fact-checking model developed by Bespoke Labs.",
        "size": "4.7GB",
        "zh_cn_msg": "由Bespoke Labs开发的最先进的事实核查模型。",
        "link": "https://ollama.com/library/bespoke-minicheck:7b",
        "parameters": "7b",
        "pull_count": "25.1K",
        "tag_count": "17",
        "updated": "6 months ago",
        "updated_time": 1728022973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "notux:8x7b",
        "name": "notux",
        "msg": "A top-performing mixture of experts model, fine-tuned with high-quality data.",
        "size": "26GB",
        "zh_cn_msg": "高性能的专家混合模型，使用高质量数据进行微调。",
        "link": "https://ollama.com/library/notux:8x7b",
        "parameters": "8x7b",
        "pull_count": "24.7K",
        "tag_count": "18",
        "updated": "15 months ago",
        "updated_time": 1704694973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "open-orca-platypus2:13b",
        "name": "open-orca-platypus2",
        "msg": "Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.",
        "size": "7.4GB",
        "zh_cn_msg": "将Open Orca OpenChat模型和Garage-bAInd Platypus 2模型合并。设计用于聊天和代码生成。",
        "link": "https://ollama.com/library/open-orca-platypus2:13b",
        "parameters": "13b",
        "pull_count": "24.2K",
        "tag_count": "17",
        "updated": "17 months ago",
        "updated_time": 1699510973,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "notus:7b",
        "name": "notus",
        "msg": "A 7B chat model fine-tuned with high-quality data and based on Zephyr.",
        "size": "4.1GB",
        "zh_cn_msg": "一个基于Zephyr并使用高质量数据进行微调的70亿参数聊天模型。",
        "link": "https://ollama.com/library/notus:7b",
        "parameters": "7b",
        "pull_count": "24K",
        "tag_count": "18",
        "updated": "15 months ago",
        "updated_time": 1704694974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "goliath:latest",
        "name": "goliath",
        "msg": "A language model created by combining two fine-tuned Llama 2 70B models into one.",
        "size": "66GB",
        "zh_cn_msg": "由两个经过微调的Llama 2 70B模型结合而成的语言模型。",
        "link": "https://ollama.com/library/goliath:latest",
        "parameters": "latest",
        "pull_count": "23.3K",
        "tag_count": "16",
        "updated": "16 months ago",
        "updated_time": 1702102974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "tulu3:8b",
        "name": "tulu3",
        "msg": "Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.",
        "size": "4.9GB",
        "zh_cn_msg": "Tülu 3 是一个领先的指令跟随模型家族，由艾伦人工智能研究所完全开源数据、代码和配方。",
        "link": "https://ollama.com/library/tulu3:8b",
        "parameters": "8b",
        "pull_count": "22.3K",
        "tag_count": "9",
        "updated": "3 months ago",
        "updated_time": 1735798974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "tulu3:70b",
        "name": "tulu3",
        "msg": "Tülu 3 is a leading instruction following model family, offering fully open-source data, code, and recipes by the The Allen Institute for AI.",
        "size": "43GB",
        "zh_cn_msg": "Tülu 3 是一个领先的指令跟随模型家族，由艾伦人工智能研究所完全开源数据、代码和配方。",
        "link": "https://ollama.com/library/tulu3:70b",
        "parameters": "70b",
        "pull_count": "22.3K",
        "tag_count": "9",
        "updated": "3 months ago",
        "updated_time": 1735798974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone-deep:2.4b",
        "name": "exaone-deep",
        "msg": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
        "size": "1.6GB",
        "zh_cn_msg": "EXAONE Deep 在包括数学和编程基准在内的各种推理任务中表现出色，其参数规模从2.4B到32B不等，由LG AI研究开发并发布。",
        "link": "https://ollama.com/library/exaone-deep:2.4b",
        "parameters": "2.4b",
        "pull_count": "21.5K",
        "tag_count": "13",
        "updated": "13 days ago",
        "updated_time": 1742451774,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone-deep:7.8b",
        "name": "exaone-deep",
        "msg": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
        "size": "4.8GB",
        "zh_cn_msg": "EXAONE Deep 在包括数学和编程基准在内的各种推理任务中表现出色，其参数规模从2.4B到32B不等，由LG AI研究开发并发布。",
        "link": "https://ollama.com/library/exaone-deep:7.8b",
        "parameters": "7.8b",
        "pull_count": "21.5K",
        "tag_count": "13",
        "updated": "13 days ago",
        "updated_time": 1742451774,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "exaone-deep:32b",
        "name": "exaone-deep",
        "msg": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.",
        "size": "19GB",
        "zh_cn_msg": "EXAONE Deep 在包括数学和编程基准在内的各种推理任务中表现出色，其参数规模从2.4B到32B不等，由LG AI研究开发并发布。",
        "link": "https://ollama.com/library/exaone-deep:32b",
        "parameters": "32b",
        "pull_count": "21.5K",
        "tag_count": "13",
        "updated": "13 days ago",
        "updated_time": 1742451774,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "firefunction-v2:70b",
        "name": "firefunction-v2",
        "msg": "An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.",
        "size": "40GB",
        "zh_cn_msg": "基于Llama 3的开源权重函数调用模型，功能上可与GPT-4相媲美。",
        "link": "https://ollama.com/library/firefunction-v2:70b",
        "parameters": "70b",
        "pull_count": "19.8K",
        "tag_count": "17",
        "updated": "8 months ago",
        "updated_time": 1722838974,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "dbrx:132b",
        "name": "dbrx",
        "msg": "DBRX is an open, general-purpose LLM created by Databricks.",
        "size": "74GB",
        "zh_cn_msg": "DBRX 是由 Databricks 创建的开放通用大型语言模型。",
        "link": "https://ollama.com/library/dbrx:132b",
        "parameters": "132b",
        "pull_count": "18.6K",
        "tag_count": "7",
        "updated": "11 months ago",
        "updated_time": 1715062974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3-guardian:2b",
        "name": "granite3-guardian",
        "msg": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
        "size": "2.7GB",
        "zh_cn_msg": "IBM Granite Guardian 3.0 2B 和 8B 模型设计用于检测提示和/或响应中的风险。",
        "link": "https://ollama.com/library/granite3-guardian:2b",
        "parameters": "2b",
        "pull_count": "17.4K",
        "tag_count": "10",
        "updated": "4 months ago",
        "updated_time": 1733206974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "granite3-guardian:8b",
        "name": "granite3-guardian",
        "msg": "The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.",
        "size": "5.8GB",
        "zh_cn_msg": "IBM Granite Guardian 3.0 2B 和 8B 模型设计用于检测提示和/或响应中的风险。",
        "link": "https://ollama.com/library/granite3-guardian:8b",
        "parameters": "8b",
        "pull_count": "17.4K",
        "tag_count": "10",
        "updated": "4 months ago",
        "updated_time": 1733206974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "alfred:40b",
        "name": "alfred",
        "msg": "A robust conversational model designed to be used for both chat and instruct use cases.",
        "size": "24GB",
        "zh_cn_msg": "一个设计用于聊天和指令场景的稳健对话模型。",
        "link": "https://ollama.com/library/alfred:40b",
        "parameters": "40b",
        "pull_count": "16.2K",
        "tag_count": "7",
        "updated": "16 months ago",
        "updated_time": 1702102974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "sailor2:1b",
        "name": "sailor2",
        "msg": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
        "size": "1.1GB",
        "zh_cn_msg": "Sailor2 是为东南亚地区打造的多语言语言模型。提供10亿、80亿和200亿参数版本。",
        "link": "https://ollama.com/library/sailor2:1b",
        "parameters": "1b",
        "pull_count": "10.4K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "sailor2:8b",
        "name": "sailor2",
        "msg": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
        "size": "5.2GB",
        "zh_cn_msg": "Sailor2 是为东南亚地区打造的多语言语言模型。提供10亿、80亿和200亿参数版本。",
        "link": "https://ollama.com/library/sailor2:8b",
        "parameters": "8b",
        "pull_count": "10.4K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "sailor2:20b",
        "name": "sailor2",
        "msg": "Sailor2 are multilingual language models made for South-East Asia. Available in 1B, 8B, and 20B parameter sizes.",
        "size": "12GB",
        "zh_cn_msg": "Sailor2 是为东南亚地区打造的多语言语言模型。提供10亿、80亿和200亿参数版本。",
        "link": "https://ollama.com/library/sailor2:20b",
        "parameters": "20b",
        "pull_count": "10.4K",
        "tag_count": "13",
        "updated": "3 months ago",
        "updated_time": 1735798974,
        "capability": [
            "llm"
        ]
    },
    {
        "full_name": "command-a:111b",
        "name": "command-a",
        "msg": "111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",
        "size": "67GB",
        "zh_cn_msg": "具有1110亿参数、针对要求严苛的企业优化的模型，这些企业需要快速、安全且高质量的人工智能。",
        "link": "https://ollama.com/library/command-a:111b",
        "parameters": "111b",
        "pull_count": "5,104",
        "tag_count": "5",
        "updated": "2 weeks ago",
        "updated_time": 1742365375,
        "capability": [
            "llm",
            "tools"
        ]
    },
    {
        "full_name": "command-r7b-arabic:7b",
        "name": "command-r7b-arabic",
        "msg": "A new state-of-the-art version of the lightweight Command R7B model that excels in advanced Arabic language capabilities for enterprises in the Middle East and Northern Africa.",
        "size": "5.1GB",
        "zh_cn_msg": "一个全新的轻量级Command R7B版本，该版本在高级阿拉伯语能力方面表现出色，适用于中东和北非地区的企业。",
        "link": "https://ollama.com/library/command-r7b-arabic:7b",
        "parameters": "7b",
        "pull_count": "4,289",
        "tag_count": "5",
        "updated": "4 weeks ago",
        "updated_time": 1741155775,
        "capability": [
            "llm",
            "tools"
        ]
    }
]